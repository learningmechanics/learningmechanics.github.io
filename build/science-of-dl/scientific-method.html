<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The scientific method in two steps and its application to deep
learning - Learning Mechanics</title>
  
  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script defer src="../static/math-render.js"></script>
  
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  
  <!-- Our minimal styles -->
  <link rel="stylesheet" href="../static/style.css">
  
  <!-- Meta tags -->
  
  <meta name="author" content="Jamie Simon">
  <meta name="date" content="2025-09-01">
</head>
<body id="top">
  <a href="/" class="home-glyph"><i class="fas fa-home"></i></a>
  <button class="font-toggle" onclick="cycleFont()"></button>
  <button class="theme-toggle" onclick="toggleTheme()"><i class="fas fa-sun"></i></button>

  <article>
    <header>
            
            <h1>The scientific method in two steps and its application
to deep learning</h1>
            <div class="sequence-nav">
        Part 2 of <a href="../science-of-dl/long-run-payoff">Notes on
the Science of Deep
Learning</a> (<a href="../science-of-dl/long-run-payoff">prev</a> | <a href="../science-of-dl/science-as-mapmaking">next</a>)
      </div>
            <hr class="title-separator">
      <div class="metadata">
        <a href="https://james-simon.github.io/">Jamie Simon</a>
        <br><time datetime="2025-09-01">2025-09-01</time>
      </div>
      <hr class="metadata-separator">
    </header>

    <main>
      <p>I have recently found myself involved in many discussions
      centered on the question of how we might develop a satisfactory
      science of modern machine learning. It certainly seems likely that
      such a science should be possible, as every important human feat
      of engineering has eventually admitted an explanatory science.
      Given such a science, engineering artifacts are revealed as points
      in a space of viable constructions: with an understanding of
      mechanics and materials science, we see the Taj Mahal and the
      Colosseum are two points in the space of stable freestanding
      structures made of stone, and we can describe the boundaries of
      this set. It is of course one of the great scientific questions of
      our time whether current foundation models and their inevitable
      successors may be located within such a scientific framework.</p>
      <p>While there is widespread support for work developing the
      ‚Äúscience of deep learning,‚Äù there is little consensus as to what
      this science will look like or what constitutes meaningful
      progress. Much of this disarray is of course inevitable and
      healthy: deep learning is complex, complementary approaches will
      be necessary, and we do not know what, exactly, we are looking
      for. Much of the confusion, however, is evitable and unhelpful:
      even when searching for an unknown object, it helps to search
      methodically. This essay is a discussion of the method of
      search.</p>
      <p>It appears to me that the present disorder lies mostly
      downstream from confusion about some basic questions: what is
      science, and how do you do it? Actually, we don‚Äôt care about
      science merely because it is Science, but rather because it is a
      technique we may use, so what we are really asking is: how do you
      make useful sense of something mysterious in the world, and when
      the mystery is great, how do you go about making progress anyways?
      While these questions are basic, they are by no means easy. I
      would like to share some thoughts on these questions informed by
      my experience and my own process of trial and error.</p>
      <p>Many great thinkers of the last century have offered insightful
      discussions of the scientific mindset and process, and I strongly
      recommend sitting down with <a
      href="https://en.wikipedia.org/wiki/Falsifiability">Popper‚Äôs
      notion of falsifiability</a>, <a
      href="https://www.lri.fr/~mbl/Stanford/CS477/papers/Kuhn-SSR-2ndEd.pdf">Kuhn‚Äôs
      depiction of the scientific process</a>, <a
      href="https://feynman.com/science/what-is-science/">Feynman‚Äôs
      joyful empiricism</a>, and <a
      href="https://www.lesswrong.com/w/original-sequences">Yudkowsky‚Äôs
      techniques for clear thinking</a>. All have greatly shaped my own
      views, and I have little to say about the general process of
      science that one of them has not already said better. I would like
      to contribute just one idea concerning the so-called scientific
      method.</p>
      <p>We learn in grade school that the process of science follows a
      defined sequence of steps: observation, hypothesis, experiment
      (with at least three trials), analysis of data, and acceptance or
      rejection of the hypothesis. However, any practicing scientist
      knows that this is not really how science works. The anatomy of a
      scientific project usually bears little resemblance to this tidy
      storybook picture.<a href="#fn1" class="footnote-ref" id="fnref1"
      role="doc-noteref"><sup>1</sup></a> Useful projects take such a
      great diversity of forms that it can seem like anything at all
      goes. One might then fairly wonder: are there, after all, any
      truly essential steps, or is one approach as good as another? This
      is an important question for any field that hopes to make material
      progress towards understanding a great mystery.</p>
      <p>It seems to me that, yes, there are two essential steps to the
      scientific method. Step A is to figure something out. Step B is to
      check and make sure you‚Äôre not wrong. You can do these steps in
      any order, but you have to do them both.</p>
      <div class="full-width-figure">
      <img src="../static/scientific_method.png" alt="The scientific method diagram" style="width: 65%;">
      <div class="figure-caption">
      <strong>Figure 1:</strong> The scientific method.
      </div>
      </div>
      <p>All the good science I know does both of these steps. If you
      only do the first step ‚Äî figuring something out but not adequately
      checking that you‚Äôre right ‚Äî you‚Äôre doing philosophy, or theology,
      or another speculative practice. Such speculation can be beautiful
      and useful, but at the end of the day, it can rarely be built
      upon. If you only do the second thing ‚Äî performing an empirical
      check but not figuring anything out ‚Äî you‚Äôre usually doing
      engineering.<a href="#fn2" class="footnote-ref" id="fnref2"
      role="doc-noteref"><sup>2</sup></a></p>
      <p>There are no rules whatsoever as to how you do the first step.
      You are allowed to figure things out via educated guess, long
      experience, mathematical derivation, meditation, or divine
      inspiration. This is often where the ingenuity of the theorist
      enters into play. There is only one rule with the second step: you
      have to do a <em>good job</em> checking you‚Äôre not wrong, ideally
      good enough to convince other people. This, too, can be done in
      many different ways ‚Äî direct experiment, elimination of
      alternatives, checking new predictions ‚Äî but it is absolutely
      essential that it is done adequately. It can be very difficult to
      do this well, or even to figure out how to do it! This is the
      nerve-wracking step where one makes sure that one isn‚Äôt fooling
      oneself. This is usually where the ingenuity of the
      experimentalist comes into play.<a href="#fn3"
      class="footnote-ref" id="fnref3"
      role="doc-noteref"><sup>3</sup></a></p>
      <p>Most of the poor science of which I am aware fails in one of
      these two steps. It is, of course, very common for a study to fail
      to adequately demonstrate its central claims: this is
      understandable, as true, interesting facts are quite difficult to
      find! On the other side, uninteresting, hypertechnical experiments
      (which have unfortunately become the norm in certain crowded areas
      of physics) perform their empirical checks just fine, but when the
      lab equipment is back in its boxes, it is unclear what general
      fact has been figured out. (Yep, our Rube Goldberg machine of
      diffraction gratings, modulators, cavities, and heavy atoms works
      as predicted! Anyone remember what it was all for?) I also see
      studies that claim to figure something out about a system of
      interest (e.g., deep learning), but which fail to state a specific
      enough conjecture to go and test it, which is a partial failure in
      both steps. (More on that later.)</p>
      <p>It is worth clarifying that a scientist does not have to do
      both steps of the scientific method in every scientific paper. An
      individual contribution may be entirely the proposal of a new
      idea, as in Darwin‚Äôs book or most of the <a
      href="https://www.chemteam.info/Chem-History/Planck-1901/Planck-1901.html">seminal</a>
      <a
      href="https://uni-tuebingen.de/fileadmin/Uni_Tuebingen/Fakultaeten/MathePhysik/Institute/IAP/Forschung/MOettel/Geburt_QM/bohr_PhilMag_26_1_1913.pdf">theoretical</a>
      <a
      href="https://link.springer.com/article/10.1007/BF01397280">physics</a>
      <a
      href="https://uni-tuebingen.de/fileadmin/Uni_Tuebingen/Fakultaeten/MathePhysik/Institute/IAP/Forschung/MOettel/Geburt_QM/schrodinger_AnnPhys_386_109_1926.pdf">papers</a>
      <a
      href="https://onlinelibrary.wiley.com/doi/10.1002/andp.19053220806">of
      the early 1900s</a>, or it may consist entirely of measurements,
      as in Tycho Brahe‚Äôs meticulous observations of the planets or <a
      href="https://www.nature.com/articles/s41586-020-2964-7">careful
      modern measurements of fundamental physical constants</a>. It is
      quite legitimate for a contribution to <em>dis</em>prove an
      existing belief without offering a replacement. The important
      thing is that a scientist doing one or the other recognizes that
      they are part of a conversation with the other part: the person
      proposing the ideas expects other people to go and test them and
      so tries to make it easy for them to do so, and the people
      checking the ideas know what the implications are if the
      experiment comes out one way or the other. Every scientific
      contribution should understand itself as part of a project that
      <em>does</em> do both steps of the scientific method.</p>
      <p>Why are both of these steps necessary for the progress of
      science? If a line of research does not purport to have figured
      out any general fact, it is unclear what has been learned or how
      to build on it. On the other hand, if it does not adequately check
      its claims, then until it does, it will carry a shadow of
      fundamental doubt that prevents others from productively building
      on it. Furthermore, and often more significantly in practice, the
      act of checking usually contains within it the act of application,
      as the most convincing way to check a claim is often to
      operationalize it to do something of interest. Science is an
      edifice that builds on itself. It usually consists of so many
      layers that each piece of brickwork must be quite solid to support
      future building, and each brick must be crafted with future bricks
      in mind.</p>
      <p>If you really believe me that the essential scientific method
      has only two steps, you might ask: what‚Äôs all this other mumbo
      jumbo about preregistered hypotheses, repeated trials, cognitive
      biases and what not? There are a few things going on. First off,
      while these two steps are simple, actually doing them is hard, so
      we have some established techniques that sometimes make them
      easier. Some of these help with the figuring out, but that‚Äôs
      mostly a dark art.<a href="#fn4" class="footnote-ref" id="fnref4"
      role="doc-noteref"><sup>4</sup></a> Most of these are techniques
      for the <em>checking</em> of our knowledge: preregistered
      hypotheses, multiple trials, the inclusion of error bars, blind
      review, and most of the other rituals of science are <em>ways to
      do step B.</em> None is essential, but they are useful techniques
      for checking our answer and avoiding fooling ourselves.</p>
      <p>Secondly, some of these techniques are field-specific. For
      example, in psychology, social science, and nutrition, it‚Äôs
      notoriously tempting to choose one‚Äôs hypothesis after seeing the
      data. In these fields, the space of hypotheses is usually large
      and the resolving power of evidence is usually weak, so choosing
      the most-supported claim post-hoc often amounts to <a
      href="https://en.wikipedia.org/wiki/Data_dredging">p-hacking</a>,
      and preregistering hypotheses makes it more likely to not be
      totally wrong. In (non-string-theory) physics, the situation is
      usually the opposite ‚Äî hypotheses are few and evidence is abundant
      ‚Äî so preregistration isn‚Äôt necessary.</p>
      <p>These steps apply even in a nascent field that knows very
      little. While no method is sufficient to guarantee useful progress
      in the search, failure to do both steps all but guarantees no
      progress will be made. By way of analogy, when searching a large
      unfamiliar house for a desired object, one‚Äôs chances of success
      are greatly improved by a methodical search that progressively
      expands an explored volume. It is possible to learn that <em>it‚Äôs
      not in this cabinet,</em> say, but only if one first identifies
      the cabinet as a useful unit of exploration, then does a
      sufficiently thorough search that the cabinet does not need to be
      revisited in the future. Failures <em>are</em> progress, but
      <em>only</em> when they are sufficiently clear and careful so as
      to reassure other searchers that the space of possible truths is
      meaningfully reduced.</p>
      <p>In summary, the scientific method consists of two steps: you
      must figure something out, and you must adequately check that you
      are not mistaken. It seems to me that these are the two things we
      must demand of any useful scientific project.</p>
      <h3
      id="how-to-use-the-scientific-method-in-the-science-of-deep-learning">How
      to use the scientific method in the science of deep learning</h3>
      <p>Deep learning presents a large number of important mysteries.
      What exactly we believe these mysteries <em>are</em> has evolved
      over time and will continue to change, but everybody agrees that
      the mysteries are there. Certainly it seems that the practice of
      deep learning involves far more arbitrary choices and yields far
      more surprising results than it ought to if we knew better. There
      is thus probably much ground to gain. Because the success of deep
      learning is an empirical phenomenon and we wish to explain it,
      this is very much a scientific question, and we will be wise to
      consciously use the methods of science to structure our
      search.</p>
      <p>We are gradually learning to do genuine science in the study of
      deep learning, and the rewards have been proportionate. However, a
      great deal of research effort ostensibly in service of our
      understanding of deep learning is expended in directions which are
      quite far from science and which consequently make little real
      progress. This is not a personal vendetta of mine: I have found
      this to be the consensus of virtually every researcher in the
      field with whom I have discussed the subject. In fact, this number
      includes many interviewees who have described their <em>own</em>
      work to me as of this ineffectual type, usually with a palpable
      air of despondence! (By contrast, the deep learning researchers I
      know who have caught the ‚Äúscience bug‚Äù tend to be energized and
      optimistic.) We as a field are due for a serious discussion of our
      methods and search strategy, and we can be optimistic that
      effectual methods are quite achievable.</p>
      <p>How can we recognize the scientific method in the study of deep
      learning? We should look for work which (a) purports to figure out
      something particular and clear about deep learning, and (b)
      reports simple, convincing experiments that compellingly support
      it. The things figured out can really take any form so long as
      they are clearly stated and seem useful. They may be empirical (‚ÄùA
      causes B‚Äù; ‚ÄúC phenomenon reliably happens‚Äù), mathematical
      (equations, limits, new mathematical objects), or even
      metascientific (e.g., ‚ÄúD is a useful proxy model for deep
      learning‚Äù). Qualitative claims are fine, but quantitative claims
      are best, because they may be verified with great confidence and
      may usually be applied in a large number of cases. Qualitative
      claims are rarely verified reliably (and often fold under later
      scrutiny) because of the sheer number of possible causes in a
      system as complex as deep learning. In assessing the progress of
      deep learning, we should count the number of interesting, easily
      verifiable quantitative claims one can make about deep learning
      systems, and individual researchers should seek to add to this
      count. This is how we will mark our progress in our search.</p>
      <p>So, how does most ‚Äúscience of deep learning‚Äù work do on this
      minimal rubric of scientific method? By way of illustration, let
      me run through some of the major research trends in the last five
      years of deep learning theory. I will start with some failings
      before ending with some victories.</p>
      <p><strong>On formality and rigor to the detriment of
      insight.</strong> It is an item of little controversy (at least
      when discussing off the record) that a great number of deep
      learning theory papers are impressively rigorous and
      mathematically complex, but ultimately shed little to no light on
      the mystery originally motivating the endeavor. Papers of this
      sort tend to share certain features: asymptotic notation obscures
      large hidden constants; theorem statements require significant
      parsing in order to extract the essence of the result; the problem
      setup introduces complexity for the sake of a more impressive
      result rather than simplifying the setup for clarity and insight;
      few or no experiments are reported, and certainly none with
      nonlinear networks. Any seasoned deep learning theorist has read
      numerous such papers.</p>
      <p>It seems to me that this pattern is the result of mistaking the
      scientific study of deep learning for a discipline of mathematics,
      which then requires formality and rigor. It emphatically is not:
      we are faced here with great and glaring empirical mysteries, and
      experiments are cheap and easy.<a href="#fn5" class="footnote-ref"
      id="fnref5" role="doc-noteref"><sup>5</sup></a> It seems virtually
      guaranteed that we will first understand deep learning through
      quick-and-dirty nonrigorous arguments which may later be
      formalized, much as a path through the woods is first blazed and
      then only later paved over in asphalt. Formality and rigor are a
      hindrance if they make it harder to understand the real nature of
      what you have figured out. As the saying goes, all theorems are
      true, but only some are interesting, and taking stock of the
      present state of our knowledge, it is greatly preferable to have
      an interesting nonrigorous result than an uninteresting
      theorem.</p>
      <p>Papers of this mathematical sort rarely include experiments:
      sometimes there are experiments tacked on at the end, but they are
      usually an afterthought. If the study of deep learning were
      mathematics, this would be understandable, as a proven theorem
      requires no empirical demonstration. Because the study of deep
      learning is a science, however, neglecting experiments almost
      always leaves easy value on the table. Unless the proven theorem
      totally resolves an important question in a completely realistic
      setting, experiments can extend a result‚Äôs scope of applicability,
      show its limits, check assumptions, or simply make it easier to
      understand the stated claim. If our goal is to understand deep
      learning, it sure seems wise to check empirically whether whatever
      you suppose you have figured out <em>applies to deep
      learning!</em> If it doesn‚Äôt, or the fit is worse than you
      expected, this merits explanation. If it does, then the
      contribution is all the greater.</p>
      <p>The overly mathematical nature of much deep learning theory
      research is natural and understandable given the field‚Äôs history:
      most workers come from TCS, statistics, or mathematics.
      Nonetheless, the game has changed, and we should require less
      rigor, more insight, and more empirics from contributions.</p>
      <p><strong>Progress in the study of the dynamics of neural network
      training.</strong> It seems to me that much of the lasting
      progress of the last five years ‚Äî the stuff that‚Äôs really stuck
      around and been built on ‚Äî is essentially all of a particular type
      which marries theory and empirics. This strain of research is
      characterized by several trends:</p>
      <ul>
      <li>It demands satisfying equations and quantitative predictions,
      and it is willing to study very simple cases and study
      dumb-seeming quantities in order to make this happen.</li>
      <li>While the equations do not usually come <em>from</em>
      experiments, they are easily verified <em>by</em>
      experiments.</li>
      <li>Assumptions are checked empirically. Assumptions that are both
      good and useful are celebrated and kept around.</li>
      <li>It is humble: it studies only what it can describe well, and
      does not make premature claims about downstream topics.</li>
      </ul>
      <p>Almost all of the work in this vein describes the
      <em>dynamics</em> of neural network learning rather than the
      <em>performance,</em> which is what I mean by humility. Some
      touchstone topics in this vein include the theories of <a
      href="https://arxiv.org/abs/1312.6120">deep linear networks</a>,
      the <a href="https://arxiv.org/abs/1711.00165">NNGP</a> and <a
      href="https://arxiv.org/abs/1806.07572">neural tangent kernel</a>,
      <a href="https://proceedings.mlr.press/v139/yang21c.html">the
      maximal update parameterization</a>, the <a
      href="https://arxiv.org/abs/2103.00065">edge of stability</a>, the
      <a href="https://arxiv.org/abs/2110.03922">generalization of
      kernel ridge regression</a>, and the study of hyperparameter
      scaling relationships (see, e.g., <a
      href="https://arxiv.org/abs/2309.16620">here</a>). All these ideas
      presented new mathematical quantities derived from model training
      which closely follow simple equations. All have proven solid
      enough to build further understanding on top of them. Since this
      vein of work includes virtually all extant examples of
      quantitatively predictive theories for deep learning, I like to
      think of it as ‚Äúexperiment-dots-on-theory-curves‚Äù or
      ‚Äúdots-on-curves‚Äù theory.</p>
      <p><strong>What about empirical science?</strong> The above
      discussion is centered largely on deep learning theory. The
      science of deep learning is a rather greater endeavor and has had
      some successes. Mechanistic interpretability cannot do what a
      physics of deep learning could (and vice versa; both are
      necessary), but it has been quite admirable as a scientific
      endeavor: it has made good use of the scientific method to
      coordinate a large-scale search over a difficult space. Deep
      learning theory should learn from it.</p>
      <p>On the even more empirical side, phenomena like adversarial
      examples and the lottery-ticket hypothesis were excellent
      empirical observations, though much of the followup work on these
      topics makes less use of the scientific method and has accreted
      into less lasting knowledge. The observation of scaling laws in
      neural network performance is perhaps the one extant example of a
      robust and important equation extracted purely from neural network
      empirics. This was an excellent observation, and it remains
      unexplained.</p>
      <p>Most ‚Äúobservations‚Äù in deep learning are of the type that ‚ÄúX
      method works for Y task.‚Äù Much fruitful dialog could be had
      between deep learning scientists and practitioners if the
      practitioners were more proactive in aggregating interesting
      phenomena and handing them to the scientists, and likewise if the
      scientists were more proactive in asking for them. Of course, most
      practitioners are too laser-focused on building AGI to care about
      theory, so I am dubious this will happen.</p>
      <p><strong>‚ÄúHail Maries.‚Äù</strong> Lastly, there have been a few
      ideas proffered of a type that I‚Äôll call ‚Äúhail Maries‚Äù after the
      long-bomb football pass. These ideas try to take a big step
      forwards all at once with a very good guess: they tend to be
      summarizable by statements of the form ‚Äúhey, what if deep learning
      is actually just X?‚Äù A good example of this is the much-embattled
      <a href="https://arxiv.org/abs/1503.02406">‚Äúinformation
      bottleneck‚Äù theory</a>. Even though the IB was conclusively
      disproven soon after its proposal, I strongly applaud the bold,
      testable hypothesis and honest attempt to figure something out.
      Attempts to jump ahead in the story like this are likely to be
      wrong, but they are very much permitted in science. Much of the
      development of quantum mechanics consisted of bold, unprecendent
      guesses! Remember that there are no rules dictating how one
      figures something out: intuition-guided guesswork is quite
      allowed. In our field, there are few ideas and much energy
      available to test them, so I would like to see more bold guesses
      of this type. We should expect to see more such leaps as time goes
      on, and some of them will turn out to be right.</p>
      <h3 id="conclusions">Conclusions</h3>
      <p>What now? We are making steady progress towards a theory of
      deep learning, a theory that we presumably hope to bend to the
      benefit of humankind. It has been over a decade since AlexNet, and
      we have tried much. Most of this has failed, but some of it has
      succeeded. It is a good time now to step back, notice the patterns
      in our successes, reassess our strategies, and seriously refocus
      our effort. Let‚Äôs get moving.</p>
      <hr />
      <section id="footnotes"
      class="footnotes footnotes-end-of-document" role="doc-endnotes">
      <hr />
      <ol>
      <li id="fn1"><p>To list some deviations I have seen firsthand:
      sometimes the hypothesis changes dramatically, or only becomes
      clear at the end, or is absent altogether. Sometimes a single
      trial suffices, and sometimes one needs millions. Sometimes the
      hypothesis and conclusions are obvious and the data gathering is
      the whole project. Sometimes the conclusion has little to do with
      the initial aims of the project. I have seen very few scientific
      projects follow the script of the ‚Äúscience fair scientific
      method,‚Äù and these few usually turned out poorly! For example,
      I‚Äôve rarely seen an interesting hypothesis confirmed by experiment
      when the scientists weren‚Äôt already damn near sure it was going to
      be true.<a href="#fnref1" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn2"><p>Of course, the creator of any engineered artifact
      can rightly claim to have ‚Äúfigured out‚Äù that such an artifact is
      possible. Sometimes this is quite an interesting discovery! The
      boundaries between engineering and science are not clear, and we
      do not need them to be.<a href="#fnref2" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn3"><p>If I were to add a third step, it would be
      convincing other people. The community is the ultimate judge of
      whether you have figured something out and whether your
      experiments show that it is not wrong. A colleague points out that
      this is similar to Dorothy Sayer‚Äôs <a
      href="https://en.wikipedia.org/wiki/The_Mind_of_the_Maker">third
      step of the creative process</a>: sharing your work with the world
      and thereby having an effect on other people. To me, peer review
      feels like an important part of the scientific process, but feels
      secondary to the scientific <em>method</em> ‚Äì even alone on a
      desert island, you could do science as I describe it here ‚Äì and in
      any case, it‚Äôs not like anyone is making important scientific
      progress and <em>not</em> sharing it, so I feel comfortable
      omitting it.<a href="#fnref3" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn4"><p>Yudkowsky attempts to make this mysterious process
      more mechanical in some of his Sequences, but it‚Äôs still quite
      difficult to come up with hypotheses.<a href="#fnref4"
      class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn5"><p>It is a very interesting question, perhaps worthy
      of discussion elsewhere, what the merits of rigor and formality
      are in an exploratory endeavor. Most obviously, a proven theorem
      is always correct and will not fail unexpectedly, so all else
      equal, a theorem is preferable to an equivalent nonrigorous claim.
      When, though, does the extra solidity justify the price in labor?
      It seems to me that rigor and formality are most useful when the
      class of objects one wishes to describe is very large and of
      unknown character, and bizarre or pathological cases are prevalent
      and important. For example, the space of groups is very large and
      diverse, and so without axioms to work from, we are lost.
      Similarly, real analysis requires formality because it turns out
      the set of all univariate functions on the reals is far stranger
      than expected, and we cannot rely on our intuitions. On the other
      hand, when one already has an intuitive feel for the set of
      objects one wishes to characterize, the guardrails of formality
      are not so necessary. It is for this reason that you need very
      little formal math to do physics. This is very much the case in
      which we find ourselves with deep learning.<a href="#fnref5"
      class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      </ol>
      </section>
    </main>
  </article>

  <hr><div class="sequence-toc"><h3>Notes on the Science of Deep Learning</h3><ol><li><a href="../science-of-dl/long-run-payoff">üöß Understanding deep learning will pay off in the long run</a></li><li><strong>The scientific method in two steps and its application to deep learning</strong></li><li><a href="../science-of-dl/science-as-mapmaking">üöß Science as mapmaking</a></li></ol></div><div class="back-to-top"><a href="#top"><i class="fas fa-arrow-circle-up"></i></a></div>

  <div class="comments">
    <h2>Comments</h2>
    <script>
      (function() {
        var theme = localStorage.getItem('theme') || 'light';
        var s = document.createElement('script');
        s.src = 'https://giscus.app/client.js';
        s.setAttribute('data-repo', 'learningmechanics/learningmechanics.github.io');
        s.setAttribute('data-repo-id', 'R_kgDOO_nSsA');
        s.setAttribute('data-category', 'Blog Comments');
        s.setAttribute('data-category-id', 'DIC_kwDOO_nSsM4Cualh');
        s.setAttribute('data-mapping', 'pathname');
        s.setAttribute('data-strict', '0');
        s.setAttribute('data-reactions-enabled', '1');
        s.setAttribute('data-emit-metadata', '0');
        s.setAttribute('data-input-position', 'bottom');
        s.setAttribute('data-theme', theme);
        s.setAttribute('data-lang', 'en');
        s.setAttribute('crossorigin', 'anonymous');
        s.async = true;
        document.currentScript.parentNode.appendChild(s);
      })();
    </script>
  </div>


  <script>
    // Font switcher
    const fonts = [
      { name: 'ET Book', family: "'et-book', 'ETBookOT', 'ET Book', Georgia, 'Times New Roman', serif" },
      { name: 'Gill Sans', family: "'Gill Sans', 'Gill Sans MT', Calibri, sans-serif" },
      { name: 'Georgia', family: "Georgia, 'Times New Roman', serif" },
      { name: 'Palatino', family: "Palatino, 'Palatino Linotype', 'Book Antiqua', serif" },
      { name: 'Garamond', family: "Garamond, 'Apple Garamond', 'Baskerville', serif" },
      { name: 'Baskerville', family: "Baskerville, 'Baskerville Old Face', 'Hoefler Text', Garamond, serif" },
      { name: 'Times', family: "'Times New Roman', Times, serif" },
      { name: 'Helvetica', family: "Helvetica, Arial, sans-serif" },
      { name: 'Comic Sans', family: "'Comic Sans MS', 'Comic Sans', cursive" },
      { name: 'Wingdings', family: "Wingdings, 'Zapf Dingbats', serif" }
    ];

    function updateFontButton(fontName) {
      const button = document.querySelector('.font-toggle');
      button.textContent = `üî§ font:  (click to change)`;
    }

    function cycleFont() {
      const currentFont = localStorage.getItem('font') || 'ET Book';
      const currentIndex = fonts.findIndex(f => f.name === currentFont);
      const nextIndex = (currentIndex + 1) % fonts.length;
      const nextFont = fonts[nextIndex];

      document.body.style.fontFamily = nextFont.family;
      localStorage.setItem('font', nextFont.name);
      updateFontButton(nextFont.name);
    }

    function toggleTheme() {
      const currentTheme = document.documentElement.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';

      document.documentElement.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);

      // Update button icon
      const icon = document.querySelector('.theme-toggle i');
      icon.className = newTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

      // Update Giscus theme
      const giscusFrame = document.querySelector('iframe.giscus-frame');
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage({
          giscus: { setConfig: { theme: newTheme } }
        }, 'https://giscus.app');
      }
    }

    // Initialize theme and font on page load
    document.addEventListener('DOMContentLoaded', function() {
      // Theme
      const savedTheme = localStorage.getItem('theme') || 'light';
      document.documentElement.setAttribute('data-theme', savedTheme);

      const icon = document.querySelector('.theme-toggle i');
      icon.className = savedTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

      // Font
      const savedFont = localStorage.getItem('font') || 'ET Book';
      const font = fonts.find(f => f.name === savedFont) || fonts[0];
      document.body.style.fontFamily = font.family;
      updateFontButton(font.name);
    });
  </script>
</body>
</html> 