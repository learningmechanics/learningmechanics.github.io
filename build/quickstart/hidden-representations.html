<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Want to understand the average size of hidden
representations? - Learning Mechanics</title>
  
  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script defer src="../static/math-render.js"></script>
  
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  
  <!-- Our minimal styles -->
  <link rel="stylesheet" href="../static/style.css">
  
  <!-- Meta tags -->
  <meta name="description" content="Exploring the mathematical
properties of hidden layer representations and their average
magnitudes.">
  <meta name="author" content="The Learning Mechanics Team">
  <meta name="date" content="2025-09-01">
</head>
<body id="top">
  <a href="../index.html" class="home-glyph"><i class="fas fa-home"></i></a>
  <button class="font-toggle" onclick="cycleFont()"></button>
  <button class="theme-toggle" onclick="toggleTheme()"><i class="fas fa-sun"></i></button>

  <article>
    <header>
            
            <h1>Want to understand the average size of hidden
representations?</h1>
            <div class="sequence-nav">
        Part 2 of <a href="../quickstart/introduction.html">A Quickstart
Guide to Learning
Mechanics</a> (<a href="../quickstart/introduction.html">prev</a> | <a href="../quickstart/hyperparameter-selection.html">next</a>)
      </div>
            <hr class="title-separator">
      <div class="metadata">
        <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">The Learning Mechanics Team</a>
        <br><time datetime="2025-09-01">2025-09-01</time>
      </div>
      <hr class="metadata-separator">
    </header>

    <main>
      <p>You should! This is foundational for understanding everything
      else about the dynamics of neural networks.</p>
      <p>Historically, the first questions people tried to answer about
      neural networks dealt with their performance and representations:
      how can we characterize how well our network performs, and what
      hidden representations do they learn as they train? We‚Äôll revisit
      these questions later in a modern light, but suffice it to say
      that they are hard and it‚Äôs unclear where to start. In rigorous
      science, it‚Äôs usually a good idea to be humble about what you can
      understand and to start with the dumbest, simplest question you
      think you can answer, working up from there. It turns out that the
      simplest useful theoretical question you can ask about neural
      networks is: <em>as you forward-propagate your signal and backprop
      your gradient, roughly how big are the (pre)activations and
      gradients on average?</em> Answering this is useful for
      understanding initialization, avoiding exploding or vanishing
      gradients, and getting stable optimization even in large
      models.</p>
      <p>More precisely: suppose we have an input vector <span
      class="math inline">\(\mathbf{x}\)</span> (an image, token, set of
      features, etc.), and we start propagating forward through our
      network, stopping part way. Denote by <span
      class="math inline">\(\mathbf{h}_\ell(\mathbf{x})\)</span> the
      hidden representations after applying the <span
      class="math inline">\(\ell\)</span>-th linear layer. What‚Äôs the
      typical size of an element of <span
      class="math inline">\(\mathbf{h}_\ell(\mathbf{x})\)</span>? If you
      want a mathematical metric for this, you might study the
      root-mean-squared size</p>
      <p><span class="math display">\[
      q_\ell(\mathbf{x}) :=
      \frac{|\!|{\mathbf{h}_\ell(\mathbf{x})}|\!|}{\sqrt{\text{size}[\mathbf{h}_\ell(\mathbf{x})]}}.
      \]</span></p>
      <p>You don‚Äôt want <span
      class="math inline">\(q_\ell(\mathbf{x})\)</span> to either blow
      up or vanish as you propagate forwards through the network. If
      either happens, you‚Äôll be feeding very large or very small
      arguments to your activation function,<a href="#fn1"
      class="footnote-ref" id="fnref1"
      role="doc-noteref"><sup>1</sup></a> which is generally a bad
      idea.<a href="#fn2" class="footnote-ref" id="fnref2"
      role="doc-noteref"><sup>2</sup></a> In a neural network of many
      layers, problems like this tend to get worse as you propagate
      through more and more layers, so you want to avoid them from the
      get go.</p>
      <h3 id="first-steps-lecun-initialization-and-large-width">First
      steps: LeCun initialization and large width</h3>
      <p>The first people to address this question seriously were
      practitioners in the 1990s. If you initialize a neural network‚Äôs
      weight parameters with unit variance ‚Äî that is, <span
      class="math inline">\(W_{ij} \sim \mathcal{N}(0,1)\)</span> ‚Äî then
      your preactivations tend to blow up. If instead you initialize
      with</p>
      <p><span class="math display">\[
      W_{ij} \sim \mathcal{N}(0, \sigma^2),
      \]</span></p>
      <p>where <span class="math inline">\(\sigma^2 =
      \frac{1}{\text{[fan in]}}\)</span>, the preactivations are better
      controlled. By a central limit theorem calculation, this init size
      ensures that well-behaved activations in the previous layer mean
      well-behaved preactivations in the following layer, at least at
      initialization.</p>
      <ul>
      <li>This scheme is now called LeCun initialization after <a
      href="https://link.springer.com/chapter/10.1007/978-3-642-35289-8_3">[LeCun
      et al.¬†(1996)]</a>.</li>
      </ul>
      <p>This is the first calculation any new deep learning scientist
      should be able to perform! Get comfortable with this kind of
      central limit theorem argument.</p>
      <p>The central limit theorem gives the best approximation for a
      sum as the number of added terms grows. This suggests that, when
      studying any sort of signal propagation problem, it‚Äôs a good and
      useful idea to consider the case of large width. <strong>This is
      the consideration that motivates the study of infinite-width
      networks, which are now central in the mathematical science of
      deep learning.</strong> It‚Äôs very important to think about this
      enough to have intuition for why the large-width limit is
      useful.</p>
      <h3
      id="infinite-width-nets-at-init-signal-propagation-and-the-neural-network-gaussian-process-nngp">Infinite-width
      nets at init: signal propagation and the neural network Gaussian
      process (NNGP)</h3>
      <p>A natural next question is: <em>what else can you say about
      wide neural networks at initialization?</em> The answer unfolds in
      the following sequence.</p>
      <p>First, you can perform a close study of the ‚Äúsignal sizes‚Äù
      <span class="math inline">\(q_\ell(\mathbf{x})\)</span> as well as
      the correlations <span class="math inline">\(c_\ell(\mathbf{x},
      \mathbf{x}&#39;) := \langle \mathbf{h}_\ell(\mathbf{x}),
      \mathbf{h}_\ell(\mathbf{x}&#39;) \rangle\)</span>. You can
      actually calculate both of these <em>exactly</em> at infinite
      width using Gaussian integrals.</p>
      <ul>
      <li>See <a href="https://arxiv.org/abs/1606.05340">[Poole et
      al.¬†(2016)]</a> for a derivation physicists will like and <a
      href="https://arxiv.org/abs/1602.05897">[Daniely et
      al.¬†(2016)]</a> for a more formal treatment better suited for
      mathematicians.
      <ul>
      <li>It‚Äôs worth getting to the point where you understand either
      Eqns (1-5) from <a href="https://arxiv.org/abs/1606.05340">[Poole
      et al.¬†(2016)]</a> or Sections 5 and 8 from <a
      href="https://arxiv.org/abs/1602.05897">[Daniely et
      al.¬†(2016)]</a>. The other stuff ‚Äî chaos, computational graphs,
      and so on ‚Äî is cool but not essential.</li>
      </ul></li>
      <li>See <a href="https://arxiv.org/abs/1611.01232">[Schoenholz et
      al.¬†(2016)]</a> for a similar size analysis for the backpropagated
      gradients.</li>
      </ul>
      <p>Next, you can study not only the <em>averages</em> of these
      quantities but also their complete <em>distributions</em>. It
      turns out they‚Äôre Gaussian at initialization (surprise, surprise)
      and the network function value itself is a ‚ÄúGaussian process‚Äù with
      a covariance kernel that you can obtain in closed form.</p>
      <ul>
      <li>This was first worked out for shallow networks by <a
      href="https://glizen.com/radfordneal/ftp/pin.pdf">[Neal
      (1996)]</a>. It took another two decades before it was extended to
      deep networks by <a href="https://arxiv.org/abs/1711.00165">[Lee
      et al.¬†(2017)]</a>. If you train only the last layer of a wide
      neural network, the resulting learning rule is equivalent to <a
      href="https://en.wikipedia.org/wiki/Kriging">Gaussian process
      regression</a> with the NNGP.</li>
      </ul>
      <p>It‚Äôs very worth working through the NNGP idea and getting
      intuition for both GPs and the forward-prop statistics that give a
      GP in this context. Notice that MLPs have NNGPs with
      rotation-invariant kernels. This will remain a useful
      intuition.</p>
      <p>At this point in our discussion, we already have papers that
      have calculated average-case quantities exactly which agree well
      with experiments using networks with widths in the hundreds or
      thousands. Look at how good the agreement is in these plots:</p>
      <div class="full-width-figure">
      <img src="../static/great_dl_th-exp_plots.png" alt="Theory-experiment agreement plots" style="width: 80%;">
      <div class="figure-caption">
      <p><b>Top: signal propagation of layerwise correlations for a deep
      <span class="math inline">\(\tanh\)</span> net from
      <a href="https://arxiv.org/abs/1606.05340">Poole et
      al.¬†(2016)</a>.</b> Blue, green, and red sets of curves correspond
      to weight initialization variances <span
      class="math inline">\(\sigma_w^2 = \{1.3, 2.3, 4.0\}\)</span>.
      Different saturations correspond to different initial
      correlations. Experiment dots lie very close to the theory curves.
      <b>Bottom: performance of NNGP regression vs.¬†ordered/chaotic
      regimes from <a href="https://arxiv.org/abs/1711.00165">Lee et
      al.¬†(2017)</a>.</b> Left subplot shows the test accuracy of GP
      regression with a depth-50 <span
      class="math inline">\(\tanh\)</span> NNGP on MNIST. Right subplot
      shows prediction of the ordered and chaotic regimes using the same
      machinery as <a href="https://arxiv.org/abs/1606.05340">Poole et
      al.¬†(2016)</a>. The best performance falls near the boundary
      between order and chaos. It‚Äôs significant that we can
      quantitatively predict the structure of a phase diagram of model
      performance, even in a simplified setting like NNGP
      regression.</p>
      </div>
      </div>
      <p>It‚Äôs worth appreciating that extremely good agreement with
      experiment is possible if we‚Äôre studying the right objects in the
      right regimes. Most deep learning theory work that can‚Äôt get
      agreement this good eventually fades or is replaced by something
      that does. It‚Äôs usually wise to insist on a quantitative match
      from your theory and be satisfied with nothing less.</p>
      <h3
      id="infinite-width-nets-under-gradient-descent-the-ntk">Infinite-width
      nets under gradient descent: the NTK</h3>
      <p>Now that we understand initialization in wide networks, we‚Äôre
      ready to study training. The first milestone on this path is the
      ‚Äúneural tangent kernel‚Äù (NTK). The main result here is that if you
      <em>train</em> a neural network in the NNGP infinite-width
      setting, its functional evolution is described by a particular
      kernel which remains static for all time. This kernel is the inner
      product of parameter gradient vectors:</p>
      <p><span class="math display">\[
      \text{NTK}(\mathbf{x}, \mathbf{x}&#39;) := \left\langle
      \nabla_{\boldsymbol{\theta}} f_{\boldsymbol{\theta}}(\mathbf{x}),
      \nabla_{\boldsymbol{\theta}}
      f_{\boldsymbol{\theta}}(\mathbf{x}&#39;) \right\rangle.
      \]</span></p>
      <p>The primary consequence is that, in this limit, the learning
      dynamics of a neural networks is, by the ‚Äúkernel trick,‚Äù
      equivalent to the dynamics of a linear model. The final learned
      function is given by <em>kernel (ridge) regression.</em></p>
      <ul>
      <li>The original NTK paper is that of <a
      href="https://arxiv.org/abs/1806.07572">[Jacot et al.¬†(2018)]</a>.
      The subsequent presentation of <a
      href="https://arxiv.org/abs/1902.06720">[Lee et al.¬†(2019)]</a> is
      less formal and may be easier for beginners.
      <ul>
      <li>The NTK idea is a bit too technical to explain here (though we
      sure want to), but it‚Äôs all but essential to understand it before
      moving on to feature learning. It‚Äôs worth allocating some time,
      working through one of these papers, and making sure you‚Äôve
      extracted the simple core idea. It‚Äôs also worth thinking carefully
      about linear models and kernel regression, as these will return
      later as first models for other learning phenomena.</li>
      <li>Notice that one can make accurate <em>quantitative</em>
      predictions for the learning of wide nets using the NTK. See, for
      example, Figure 2 of <a
      href="https://arxiv.org/abs/1902.06720">[Lee et
      al.¬†(2019)]</a>.</li>
      </ul></li>
      </ul>
      <p>Motivated by the NTK, people found new and clever ways to ask
      and answer for kernel regression the questions we <em>want</em>
      answered of deep learning. This gave the field a useful new tool
      and has led to some moderately valuable insights. For example,
      networks in the NTK limit always converge to zero training loss so
      long as the learning rate isn‚Äôt too big, and this served as a
      useful demonstration of how overparameterization usually makes
      optimization <em>easier,</em> not harder. Kernel models will
      appear a few other times in other chapters of this guide.</p>
      <h3
      id="scaling-analysis-of-feature-evolution-the-maximal-update-parameterization-mup">Scaling
      analysis of feature evolution: the maximal update parameterization
      (<span class="math inline">\(\mu\)</span>P)</h3>
      <p>After the development of the NTK, people quickly noticed that
      networks in this limit don‚Äôt exhibit feature learning. That is, at
      infinite width, the hidden neurons of a network represent the same
      functions after training as they did at initialization. At
      large-but-finite width, the change is finite but negligible. This
      is a first clue that the pretty, analytically tractable NTK limit
      isn‚Äôt the end of the story.<a href="#fn3" class="footnote-ref"
      id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
      <p>For a few years, it seemed like we might have to give up on
      infinite-width networks. Fortunately, it turned out that there‚Äôs
      <em>another</em> coherent infinite-width limit in which things
      scale differently, and the network <em>does</em> actually undergo
      feature learning. This is the regime in which most deep learning
      theory now takes place.</p>
      <p>Here‚Äôs the evolution of ideas, some key papers, and key
      takeaways:</p>
      <ul>
      <li><a href="https://arxiv.org/abs/1812.07956">[Chizat and Bach
      (2018)]</a> gave a ‚Äúlazy‚Äù vs.¬†‚Äúrich‚Äù dichotomy of gradient descent
      dynamics, using ‚Äúlazy‚Äù to mean ‚Äúkernel dynamics‚Äù and ‚Äúrich‚Äù to
      mean ‚Äúanything else.‚Äù
      <ul>
      <li>It‚Äôs worth understanding what ‚Äúlazy‚Äù training is, and how an
      output multiplier can make any neural network train lazily.</li>
      </ul></li>
      <li><a href="https://arxiv.org/abs/1804.06561">[Mei et al
      (2018)]</a>, <a href="https://arxiv.org/abs/1805.00915">[Rotskoff
      and Vanden-Eijnden (2018)]</a>, and a handful of other authors
      pointed out a ‚Äúmean-field‚Äù parameterization that allows
      infinite-width shallow neural nets to learn features. They give it
      different names but describe the same parameterization.
      <ul>
      <li>This may be an easier place to start than <span
      class="math inline">\(\mu\)</span>P, but if you understand <span
      class="math inline">\(\mu\)</span>P, you can skip mean-field nets
      for now.</li>
      </ul></li>
      <li><a
      href="https://proceedings.mlr.press/v139/yang21c.html">[Yang and
      Hu (2021)]</a> extended this insight to deep networks. They
      pointed out a way that layerwise init sizes + learning rates can
      scale with network width so that a deep network learns features to
      leading order even at infinite width. They call this the ‚Äúmaximal
      update parameterization‚Äù (muP, or <span
      class="math inline">\(\mu\)</span>P).
      <ul>
      <li>The core idea here is extremely important and can be
      understood in simple terms. Their ‚ÄúTensor Programs‚Äù and
      ‚Äúabc-param‚Äù frameworks are fairly complicated ‚Äî you don‚Äôt need to
      understand either to get the main gist. <a
      href="https://arxiv.org/abs/2310.17813">[Yang et al.¬†(2023)]</a>
      give a simpler framing of the big idea here, and <a
      href="https://arxiv.org/abs/2404.19719">[Karkada (2024)]</a> give
      a beginner-friendly tutorial of the NTK/<span
      class="math inline">\(\mu\)</span>P story.</li>
      </ul></li>
      <li><a href="https://arxiv.org/abs/2203.03466">[Yang et
      al.¬†(2022)]</a> showed that this parameterization is practically
      useful: it lets one scale up neural networks while preserving
      network hyperparameters. (More on this in <a
      href="hyperparameter-selection.html">our discussion of
      hyperparameters</a>.)
      <ul>
      <li>This is probably the first practically impactful achievement
      of deep learning theory in the modern era, so it‚Äôs worth thinking
      about seriously.</li>
      </ul></li>
      </ul>
      <p>These ‚Äúrich,‚Äù feature learning, <span
      class="math inline">\(\mu\)</span>P dynamics led to a paradigm
      shift in deep learning theory. Most later work uses or relates to
      it in some way. It‚Äôs thus very important to understand. A seasoned
      deep learning theorist should be able to sit down and derive the
      <span class="math inline">\(\mu\)</span>P parameterization, or
      something equivalent to it, from first principles. It‚Äôs difficult
      to do relevant work in 2025 without it!</p>
      <p>It‚Äôs worth noting that, unlike the NTK limit, the <span
      class="math inline">\(\mu\)</span>P limit is very difficult to
      study analytically. In the NTK limit, we have kernel behavior,
      simple gradient descent dynamics, a convex loss surface (using
      squared loss), and lots of older theoretical tools that we can
      bring to bear. In the <span class="math inline">\(\mu\)</span>P
      limit, we have none of this. To our knowledge, nobody‚Äôs even shown
      a general result that a deep network in the <span
      class="math inline">\(\mu\)</span>P limit <em>converges,</em> let
      alone characterized the solution that‚Äôs found!</p>
      <div class="question-box" id="oq-2-1">
      <p><strong>Open Question 2.1: Convergence of wide <span
      class="math inline">\(\mu\)</span>P networks.</strong> Under what
      conditions does a network in the infinite-width <span
      class="math inline">\(\mu\)</span>P limit converge when optimized
      with gradient descent?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <p>In the NTK limit, we can study the model with the
      well-established math of kernel theory, which already existed and
      has now been developed further expressly for the study of the NTK.
      In the <span class="math inline">\(\mu\)</span>P limit, the best
      we have so far are rather complex calculational frameworks:</p>
      <ul>
      <li>The dynamical mean-field theory (DMFT) frameworks of <a
      href="https://arxiv.org/abs/2205.09653">[Bordelon and Pehlevan
      (2022)]</a> and <a href="https://arxiv.org/abs/2402.03220">[Dandi
      et al.¬†(2024)]</a> let one compute feature distributions of
      infinite-width networks in the rich regime, but it is quite
      complicated, and it is difficult to extract analytical insight.
      (It is nonetheless useful for scaling calculations.) The Tensor
      Programs framework of <a
      href="https://proceedings.mlr.press/v139/yang21c.html">[Yang and
      Hu (2021)]</a> allows one to perform the same calculations with
      random matrix theory language.
      <ul>
      <li>These are specialized tools and are not essential on a first
      pass through deep learning theory.</li>
      </ul></li>
      </ul>
      <div class="question-box" id="oq-2-2">
      <p><strong>Open Question 2.2: Framework for studying feature learning
      at large width.</strong> is there a simple, computationally
      tractable calculational framework ‚Äî potentially making realistic
      simplifying assumptions ‚Äî that allows us to quantitatively study
      feature evolution of a general class of neural network in the rich
      regime and which requires tracking less information than the DMFT
      framework of <a href="https://arxiv.org/abs/2205.09653">[Bordelon
      and Pehlevan (2022)]</a>?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <h3 id="onwards-towards-infinite-depth">Onwards: towards infinite
      depth</h3>
      <p>Early in this chapter, we took width to infinity, which allowed
      us a host of useful calculational tools. We can also take depth to
      infinity. There are several ways to do this, but the upshot is
      that one quickly encounters stability problems with a standard
      MLP, so a ResNet formulation in which each layer gets a small
      premultiplier seems like the most promising choice.</p>
      <ul>
      <li><a href="https://arxiv.org/abs/2309.16620">[Bordelon et
      al.¬†(2023)]</a> and <a
      href="https://arxiv.org/abs/2310.02244">[Yang et al.¬†(2023)]</a>
      give treatments of <span class="math inline">\(\mu\)</span>P at
      large depth. They conclude with slightly different scaling
      recommendations for layerwise premultipliers.
      <ul>
      <li>This is probably useful to understand for the future but is
      not yet essential knowledge. As with <span
      class="math inline">\(\mu\)</span>P, the most important thing to
      get out of either paper is the scaling calculation that gives the
      hyperparameter prescription.</li>
      </ul></li>
      </ul>
      <div class="question-box" id="oq-2-3">
      <p><strong>Open Question 2.3: Framework for studying feature learning
      at large width and depth.</strong> is there a simple,
      computationally tractable calculational framework ‚Äî potentially
      making realistic simplifying assumptions ‚Äî that allows us to
      quantitatively study the feature evolution of an
      infinite-<em>depth</em> network in the rich regime?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <section id="footnotes"
      class="footnotes footnotes-end-of-document" role="doc-endnotes">
      <hr />
      <ol>
      <li id="fn1"><p>‚Ä¶or feeding very large or small values to a norm
      layer, or representing them in finite precision and losing bits,
      or some other malady.<a href="#fnref1" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn2"><p>To see why we want order-one preactivations,
      imagine feeding either very small or very large values into the
      function <span class="math inline">\(\phi(z) = \tanh(z)\)</span>.
      If the values are small, <span class="math inline">\(\phi(z)
      \approx z\)</span>, and we‚Äôve lost the nonlinearity we wanted. If
      the values are large, then <span class="math inline">\(\phi(z)
      \approx \operatorname{sign}(z)\)</span>, and backpropagated
      gradients will be almost zero. With a homogeneous activation
      function like <span class="math inline">\(\text{ReLU}\)</span>, we
      could in principle have very small or very large preactivations,
      but a careful study of the dynamics reveals that we don‚Äôt actually
      gain anything by doing this, so we might as well always insist
      that preactivations should be order-one in distribution.<a
      href="#fnref2" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn3"><p>There was a great deal of debate in this period as
      to whether the NTK limit of neural networks is a ‚Äúgood‚Äù or ‚Äúbad‚Äù
      model for deep learning. In reality, for the phenomena it
      captures, it‚Äôs just about the simplest, most tractable model that
      does, and it should usually be the first case you study. For the
      phenomena it doesn‚Äôt capture, it‚Äôs of course a bad model, and you
      can‚Äôt study it to gain understanding. You should understand the
      NTK enough that you‚Äôre wise enough to tell the difference, then
      learn the handful of useful phenomena it captures well.<a
      href="#fnref3" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      </ol>
      </section>
    </main>
  </article>

  <hr><div class="sequence-toc"><h3>A Quickstart Guide to Learning Mechanics</h3><ol><li><a href="../quickstart/introduction.html">Introduction: what do you want to understand?</a></li><li><strong>...the average size of hidden representations?</strong></li><li><a href="../quickstart/hyperparameter-selection.html">...hyperparameter selection (and why should theorists care)?</a></li><li><a href="../quickstart/optimization.html">üöß ...the convergence and stability of optimization?</a></li><li><a href="../quickstart/feature-learning.html">üöß ...feature learning and the final network weights?</a></li><li><a href="../quickstart/generalization.html">üöß ...generalization?</a></li><li><a href="../quickstart/sparsity.html">üöß ...neuron-level sparsity?</a></li><li><a href="../quickstart/data-structure.html">üöß ...the structure in the data?</a></li><li><a href="../quickstart/conclusion.html">üöß Places to make a difference</a></li></ol></div><div class="back-to-top"><a href="#top"><i class="fas fa-arrow-circle-up"></i></a></div>

  <div class="comments">
    <h2>Comments</h2>
    <script>
      (function() {
        var theme = localStorage.getItem('theme') || 'light';
        var s = document.createElement('script');
        s.src = 'https://giscus.app/client.js';
        s.setAttribute('data-repo', 'learningmechanics/learningmechanics.github.io');
        s.setAttribute('data-repo-id', 'R_kgDOO_nSsA');
        s.setAttribute('data-category', 'Blog Comments');
        s.setAttribute('data-category-id', 'DIC_kwDOO_nSsM4Cualh');
        s.setAttribute('data-mapping', 'pathname');
        s.setAttribute('data-strict', '0');
        s.setAttribute('data-reactions-enabled', '1');
        s.setAttribute('data-emit-metadata', '0');
        s.setAttribute('data-input-position', 'bottom');
        s.setAttribute('data-theme', theme);
        s.setAttribute('data-lang', 'en');
        s.setAttribute('crossorigin', 'anonymous');
        s.async = true;
        document.currentScript.parentNode.appendChild(s);
      })();
    </script>
  </div>


  <script>
    // Font switcher
    const fonts = [
      { name: 'ET Book', family: "'et-book', 'ETBookOT', 'ET Book', Georgia, 'Times New Roman', serif" },
      { name: 'Gill Sans', family: "'Gill Sans', 'Gill Sans MT', Calibri, sans-serif" },
      { name: 'Georgia', family: "Georgia, 'Times New Roman', serif" },
      { name: 'Palatino', family: "Palatino, 'Palatino Linotype', 'Book Antiqua', serif" },
      { name: 'Garamond', family: "Garamond, 'Apple Garamond', 'Baskerville', serif" },
      { name: 'Baskerville', family: "Baskerville, 'Baskerville Old Face', 'Hoefler Text', Garamond, serif" },
      { name: 'Times', family: "'Times New Roman', Times, serif" },
      { name: 'Helvetica', family: "Helvetica, Arial, sans-serif" },
      { name: 'Comic Sans', family: "'Comic Sans MS', 'Comic Sans', cursive" },
      { name: 'Wingdings', family: "Wingdings, 'Zapf Dingbats', serif" }
    ];

    function updateFontButton(fontName) {
      const button = document.querySelector('.font-toggle');
      button.textContent = `üî§ font:  (click to change)`;
    }

    function cycleFont() {
      const currentFont = localStorage.getItem('font') || 'ET Book';
      const currentIndex = fonts.findIndex(f => f.name === currentFont);
      const nextIndex = (currentIndex + 1) % fonts.length;
      const nextFont = fonts[nextIndex];

      document.body.style.fontFamily = nextFont.family;
      localStorage.setItem('font', nextFont.name);
      updateFontButton(nextFont.name);
    }

    function toggleTheme() {
      const currentTheme = document.documentElement.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';

      document.documentElement.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);

      // Update button icon
      const icon = document.querySelector('.theme-toggle i');
      icon.className = newTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

      // Update Giscus theme
      const giscusFrame = document.querySelector('iframe.giscus-frame');
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage({
          giscus: { setConfig: { theme: newTheme } }
        }, 'https://giscus.app');
      }
    }

    // Initialize theme and font on page load
    document.addEventListener('DOMContentLoaded', function() {
      // Theme
      const savedTheme = localStorage.getItem('theme') || 'light';
      document.documentElement.setAttribute('data-theme', savedTheme);

      const icon = document.querySelector('.theme-toggle i');
      icon.className = savedTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

      // Font
      const savedFont = localStorage.getItem('font') || 'ET Book';
      const font = fonts.find(f => f.name === savedFont) || fonts[0];
      document.body.style.fontFamily = font.family;
      updateFontButton(font.name);
    });
  </script>
</body>
</html> 