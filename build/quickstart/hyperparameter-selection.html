<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Want to understand hyperparameter selection (and why should
theorists care)? - Learning Mechanics</title>
  
  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script defer src="../static/math-render.js"></script>
  
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  
  <!-- Our minimal styles -->
  <link rel="stylesheet" href="../static/style.css">
  
  <!-- Meta tags -->
  <meta name="description" content="The theoretical foundations of
hyperparameter optimization and its importance for understanding deep
learning.">
  <meta name="author" content="The Learning Mechanics Team">
  <meta name="date" content="2025-09-01">
</head>
<body id="top">
  <a href="../index.html" class="home-glyph"><i class="fas fa-home"></i></a>
  <button class="font-toggle" onclick="cycleFont()"></button>
  <button class="theme-toggle" onclick="toggleTheme()"><i class="fas fa-sun"></i></button>

  <article>
    <header>
            
            <h1>Want to understand hyperparameter selection (and why
should theorists care)?</h1>
            <div class="sequence-nav">
        Part 3 of <a href="../quickstart/introduction.html">A Quickstart
Guide to Learning
Mechanics</a> (<a href="../quickstart/hidden-representations.html">prev</a> | <a href="../quickstart/optimization.html">next</a>)
      </div>
            <hr class="title-separator">
      <div class="metadata">
        <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">The Learning Mechanics Team</a>
        <br><time datetime="2025-09-01">2025-09-01</time>
      </div>
      <hr class="metadata-separator">
    </header>

    <main>
      <p>Training a neural network is a strange and mysterious process:
      a bewildering cavalcade of tensors is randomly initialized and
      subject to repeated gradient updates, and as a consequence of
      these simple operations, the whole thing <em>learns</em>.
      Meanwhile, before the process begins, you have to set your
      learning rate and some other fiddly knobs and dials. In comparison
      with the weights themselves, these hyperparameters might seem
      drab, technical and mundane; does a serious deep learning theorist
      really need to bother with them? It turns out the answer is
      emphatically ‚Äúyes‚Äù: not only is the study of hyperparameters the
      easiest place for theory to make a practical impact, it‚Äôs also
      essential for the rest of the field, since any hyperparameter you
      can‚Äôt control for will interfere with your attempts to study
      anything else.</p>
      <p>While most hyperparameters are still waiting for good theory,
      we understand a few. We‚Äôll explain how theorists currently think
      about hyperparameters, list off the successes, and point out some
      frontiers. This chapter will be comparatively long, so we‚Äôll start
      with a table of contents.</p>
      <div class="sequence-toc">
      <h3>
      Want to understand hyperparameter selection?
      </h3>
      <ol>
      <li>
      <a href="#classes-of-hyperparameter-optimization-architecture-and-data">Classes
      of hyperparameter: optimization, architecture, and data</a>
      </li>
      <li>
      <a href="#how-to-deal-with-hyperparameters-as-a-theorist">How to
      deal with hyperparameters as a theorist</a>
      </li>
      <li>
      <a href="#recurring-motif-hyperparameter-scaling-relationships">Recurring
      motif: hyperparameter scaling relationships</a>
      </li>
      <li>
      <a href="#width-initialization-scale-and-learning-rate">Width,
      initialization scale, and learning rate</a>
      </li>
      <li>
      <a href="#wider-is-better">‚ÄúWider is better‚Äù</a>
      </li>
      <li>
      <a href="#depth">Depth</a>
      </li>
      <li>
      <a href="#batch-size">Batch size</a>
      </li>
      <li>
      <a href="#transformer-specific-hyperparameters">Transformer-specific
      hyperparameters</a>
      </li>
      <li>
      <a href="#activation-function">Activation function</a>
      </li>
      <li>
      <a href="#new-frontiers">New frontiers</a>
      </li>
      </ol>
      </div>
      <h3
      id="classes-of-hyperparameter-optimization-architecture-and-data">Classes
      of hyperparameter: optimization, architecture, and data</h3>
      <p>By <em>hyperparameters,</em> we mean all the numbers and
      choices that define our training process. Unlike the network
      <em>parameters</em> ‚Äî the tensor weights optimized during training
      ‚Äî hyperparameters are chosen at the start of training and don‚Äôt
      change. Together, the hyperparameters specify the optimization
      process, the architecture, and the preprocessing of the dataset.
      The choice of hyperparameters can make a big difference in model
      performance, and unless you have a smart way to choose them, it‚Äôs
      not uncommon to spend 10-100x the effort and compute you‚Äôd expend
      training the final model just optimizing hyperparameters. A little
      math can make this search much more methodical, and thus the
      science of hyperparameters is up and away the most practically
      impactful area of deep learning theory in 2025.</p>
      <p>Even a simple neural network training procedure involves a
      dizzying array of hyperparameters. We will narrow in on only a few
      key hyperparameters for analytical study, but here at the outset,
      it‚Äôs worth enumerating a full list to get a sense of scope.</p>
      <p>Hyperparameters can be grouped into three categories. First,
      <strong>optimization hyperparameters</strong> dictate how network
      parameters are initialized and respond to gradient.</p>
      <ol type="1">
      <li><strong>The optimizer</strong> ‚Äî SGD, Adam, Muon, or similar ‚Äî
      fixes the functional form by which parameter <em>gradients</em>
      are turned into parameter <em>updates</em>.</li>
      <li>The optimizer will have one or more hyperparameters ‚Äî
      <strong>learning rate <span class="math inline">\(\eta\)</span>,
      momentum <span class="math inline">\(\mu\)</span>, weight decay
      <span class="math inline">\(\text{wd}\)</span>, tolerance <span
      class="math inline">\(\epsilon\)</span>,</strong> etc. ‚Äî that
      enter as constants in the function above. These parameters affect
      the dynamics of optimization.</li>
      <li>The <strong>batch size</strong> <span
      class="math inline">\(B\)</span> and <strong>total step count
      <span class="math inline">\(T\)</span></strong> specify the amount
      of data in each gradient batch and the total number of batches to
      process.</li>
      <li>The <strong>initialization scale</strong> <span
      class="math inline">\(\sigma\)</span> for each layer specifies the
      size of parameters at init.</li>
      </ol>
      <p>Optimization hyperparameters tend to be the most
      <em>quantitative:</em> many are real-valued numbers instead of
      choices from a discrete set. As a result, they are the most
      amenable to theoretical analysis and will receive most of our
      attention in this chapter.<a href="#fn1" class="footnote-ref"
      id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
      <p><strong>Architectural hyperparameters</strong> dictate the
      structure of the network‚Äôs forward computation. These include the
      architecture type (MLP, CNN, transformer, etc.), the number of
      layers and their widths, the presence and location of norm layers,
      the choice of nonlinear activation function, the floating point
      resolution, and any other quirks of the network structure. The
      space of possible architectures is large and ill-defined, and it
      remains difficult to search over methodically. Of these
      hyperparameters, we will mostly discuss only network
      <strong>width,</strong> <strong>depth,</strong> and
      <strong>activation function</strong>, as these are the ones we
      currently know how to study.</p>
      <p>Lastly, <strong>data hyperparameters</strong> include the
      choice of dataset, any cleaning, curation or tokenization
      procedures, any choice of curriculum, and any fine-tuning
      procedure. Though interesting and crucial, these choices are
      beyond the reach of current theory, and we will mostly omit them
      from the rest of the chapter.</p>
      <h3 id="how-to-deal-with-hyperparameters-as-a-theorist">How to
      deal with hyperparameters as a theorist</h3>
      <p>A practitioner concerned only with model performance can
      optimize hyperparameters numerically and forget about them. A
      theorist ought to be concerned with more than model performance,
      though, and should try to predict quantities that will be affected
      by these hyperparameters. For example, the loss (or sharpness, or
      feature change, etc.) after <span class="math inline">\(T\)</span>
      steps will usually depend on the learning rate <span
      class="math inline">\(\eta\)</span> (and a whole lot else), so any
      quantative prediction of the type we would like to make will
      depend explicitly on <span class="math inline">\(\eta\)</span> and
      other hyperparameters. At first, this might seem to spell doom for
      our hopes for simple theory. What can we do?</p>
      <p>There are two main answers. The first and easiest is to simply
      remove any hyperparameter you can. If you can do your study
      without a hyperparameter ‚Äî momentum, say, or norm layers ‚Äî do it;
      you can always add them back later, and your science will be
      clearer with fewer bells and whistles in the way. If your
      optimization process has a single unique minimizer reached no
      matter the learning rate, make sure you train for long enough to
      reach it. Doing this, you can usually reduce the problem to a few
      optimization hyperparameters (e.g.¬†learning rate(s), batch size,
      and step count) and a few architectural hyperparameters
      (e.g.¬†width, depth, activation function, init scale).</p>
      <h3
      id="recurring-motif-hyperparameter-scaling-relationships">Recurring
      motif: hyperparameter scaling relationships</h3>
      <p>After removing all the hyperparameters you can, you should look
      for <em>scaling relationships</em> between your hyperparameters
      that let you reduce the effective number. For example, if you map
      <span class="math inline">\((\eta, T) \mapsto (\frac{1}{2} \eta, 2
      T)\)</span> ‚Äî that is, you halve the learning rate and double the
      step count ‚Äî then you approximately get the same training
      dynamics, so long as <span class="math inline">\(\eta\)</span> was
      small enough to begin with. This ‚Äúgradient flow‚Äù limit is very
      useful: unless finite stepsize effects are important for your
      study, you should work in it. In this limit, we only need to care
      about the ‚Äúeffective training time‚Äù <span
      class="math inline">\(\tau := \eta \cdot T\)</span> and can forget
      <span class="math inline">\(\eta\)</span> and <span
      class="math inline">\(T\)</span> as independent quantities, so
      we‚Äôve effectively reduced our number of hyperparameters by
      one.</p>
      <p>The same is true of large width. In the <a
      href="hidden-representations.html">previous chapter</a>, we
      discussed how for a middle layer of a moderately wide MLP, the
      parameters should be initialized with scale <span
      class="math inline">\(\sigma \sim
      \frac{1}{\sqrt{\text{width}}}\)</span>. If you quadruple width,
      you should halve the init scale. If you also adjust the learning
      rate in accordance with <span class="math inline">\(\mu\)</span>P,
      you can work in the large-width limit, and width vanishes as a
      hyperparameter. Unless finite width is important for your study,
      you should probably work mathematically in the large-width limit
      (and of course compare empirically to finite-width nets).</p>
      <p>Since the experiments you run to match your theory won‚Äôt take
      place in an infinite or infinitesimal limit, we still have the
      question of <em>how small is small enough</em> (and ditto for
      ‚Äúlarge‚Äù). Fortunately, numerical evidence suffices for this:
      divide <span class="math inline">\(\eta\)</span> by another factor
      of 10, or multiply width by another factor of 10, and if the
      change in dynamics is negligible, you‚Äôre close enough to the
      limit.<a href="#fn2" class="footnote-ref" id="fnref2"
      role="doc-noteref"><sup>2</sup></a></p>
      <p>Almost all of our useful understanding of hyperparameters takes
      the form of hyperparameter scaling relationships.<a href="#fn3"
      class="footnote-ref" id="fnref3"
      role="doc-noteref"><sup>3</sup></a> There are probably several
      important scaling relationships that remain to be worked out.
      After using all known relationships, you are usually still left
      with a handful of hyperparameters. The number of remaining
      hyperparameters often determines the difficulty of the calculation
      you have to do, so it‚Äôs a good idea to get it as low as
      possible.<a href="#fn4" class="footnote-ref" id="fnref4"
      role="doc-noteref"><sup>4</sup></a></p>
      <p>Without further ado, here are the hyperparameters we have
      theory for.</p>
      <h3 id="width-initialization-scale-and-learning-rate">Width,
      initialization scale, and learning rate</h3>
      <p>This was mostly covered in the <a
      href="hidden-representations.html">previous section</a>. There‚Äôs
      essentially only one way to scale the layerwise init sizes and
      learning rates with model width such that you retain feature
      learning at large width. This scaling scheme is called the
      <em>maximal-update parameterization,</em> or <span
      class="math inline">\(\mu\)</span>P.</p>
      <ul>
      <li>The original paper here is <a
      href="https://proceedings.mlr.press/v139/yang21c.html">[Yang and
      Hu (2021)]</a>. Most people find that <a
      href="https://arxiv.org/abs/2310.17813">[Yang et al.¬†(2023)]</a>
      gives a simpler exposition. The core idea here is essential; this
      is the only hyperparameter scaling relationship in this section
      that‚Äôs mandatory for doing or reading most modern deep learning
      theory.</li>
      <li><a href="https://arxiv.org/abs/2203.03466">[Yang et
      al.¬†(2022)]</a>‚Äôs followup ‚Äú<span
      class="math inline">\(\mu\)</span>Transfer‚Äù paper showed that
      getting the scaling relationships here right can let you optimize
      your hyperparameters on a small model and scale them up to a large
      model, much like how civil engineers build scaled-down models to
      test the mechanics of proposed designs. This paper basically
      launched the modern study of hyperparameter scaling and is one of
      very few practically influential theory papers to date.</li>
      </ul>
      <p>It‚Äôs worth noting that there are order-one constant prefactors
      at each layer that <span class="math inline">\(\mu\)</span>P
      leaves undetermined. For example, <span
      class="math inline">\(\mu\)</span>P tells us that the init scale
      for an intermediate layer should be such that <span
      class="math inline">\(\sigma_\text{eff} := \sigma *
      \sqrt{\text{width}}\)</span> is an order-one, width-independent
      quantity, but it doesn‚Äôt tell us what the actual value should be.
      There is currently no theory that tells us what these should
      be.</p>
      <div class="question-box" id="oq-3-1">
      <p><strong>Open Question 3.1: Optimal hyperparameters for a simple
      nonlinear model.</strong> In a simple but nontrivial model ‚Äî say,
      a linear network of infinite width but finite depth, trained with
      population gradient descent ‚Äî what are the optimal choices for the
      layerwise init scales and learning rates ‚Äì not just the width
      scalings but also the constant prefactors? Are they the same or
      different between layers? Do empirics reveal discernible patterns
      that theory might aim to explain?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <div class="question-box" id="oq-3-2">
      <p><strong>Open Question 3.2: Scaling relationships for learning rate
      schedules.</strong> What scaling rules or relationships apply to
      learning rate schedules? What nondimensionalized quantities
      emerge? Can we ‚Äúpost-dict‚Äù properties of common learning rate
      schedules used in practice?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <p>The distinction between the ‚Äúlazy‚Äù NTK regime and ‚Äúrich‚Äù <span
      class="math inline">\(\mu\)</span>P regime can be boiled down to a
      single hyperparameter <span class="math inline">\(\gamma\)</span>
      that appears as an output multiplier on the network. This
      ‚Äúrichness‚Äù hyperparameter dictates how much hidden representations
      much change to effect an order-one change in the network output.
      This is an interesting hyperparameter to tune in its own right:
      <span class="math inline">\(\mu\)</span>P prescribes that <span
      class="math inline">\(\gamma\)</span> should be a
      width-independent constant, but the actual value of this constant
      significantly affects the dynamics of training. Smaller <span
      class="math inline">\(\gamma\)</span> causes lazier training,
      weaker feature evolution, and more kernel-like training dynamics.
      At larger <span class="math inline">\(\gamma\)</span>, we start to
      see steps and plateaus in the loss curve. There‚Äôs a great deal of
      interesting and poorly-understood behavior in this ‚Äúultra-rich
      regime,‚Äù and some new ideas will be needed to understand it.</p>
      <ul>
      <li><a href="https://arxiv.org/abs/2410.04642">[Atanasov et
      al.¬†(2024)]</a> found how the global learning rate should scale
      with <span class="math inline">\(\gamma\)</span>. The scaling
      exponents are different at large <span
      class="math inline">\(\gamma\)</span>.
      <ul>
      <li>This is a good first paper to read on the large-<span
      class="math inline">\(\gamma\)</span> regime. If you can
      understand Figure 1, you have the important ideas.</li>
      </ul></li>
      </ul>
      <div class="question-box" id="oq-3-3">
      <p><strong>Open Question 3.3: Is richer better?</strong> <a
      href="https://arxiv.org/abs/2410.04642">[Atanasov et
      al.¬†(2024)]</a> find that, in online training, networks with
      larger richness parameter <span
      class="math inline">\(\gamma\)</span> generalize better (so long
      as they‚Äôre given enough training time to escape the initial
      plateau). Is this generally true? Why?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <p>The ultra-rich regime is essentially the same as the
      small-initialization or ‚Äúsaddle-to-saddle‚Äù regime, which has been
      studied since before <span class="math inline">\(\mu\)</span>P. -
      <a href="https://arxiv.org/abs/2106.15933">[Jacot et
      al.¬†(2021)]</a> coin the term ‚Äúsaddle-to-saddle‚Äù and describe the
      evolution of deep linear networks in this regime. They find a
      ‚Äúgreedy low-rank dynamics‚Äù and stepwise loss curves similar to
      those later seen in the ultra-rich regime. We will revisit
      saddle-to-saddle dynamics in the <a href="optimization.html">next
      chapter</a>.</p>
      <h3 id="wider-is-better">‚ÄúWider is better‚Äù</h3>
      <p>Whenever we take a limit and thereby simplify our model, we
      ought to ask whether we‚Äôve lost any essential behavior. In the
      case of infinite width, it‚Äôs generally believed that infinite
      width nets outperform finite width nets on realistic tasks when
      the hyperparameters are properly tuned, which suggests that the
      core phenomena of deep learning we wish to explain are still there
      in the limit.</p>
      <ul>
      <li><a href="https://arxiv.org/abs/2203.03466">[Yang et
      al.¬†(2022)]</a> found empirically that larger models perform
      strictly better when scaled up with <span
      class="math inline">\(\mu\)</span>P.</li>
      <li><a href="https://arxiv.org/abs/2311.14646">[Simon et
      al.¬†(2024)]</a> showed analytically that wider is better for a
      random feature model (i.e., a shallow net with the second layer
      trained).</li>
      </ul>
      <div class="full-width-figure">
      <img src="../static/mutransfer_plot.png" alt="Width transfer demonstration" style="width: 35%;">
      <div class="figure-caption">
      <strong>Figure 1:</strong> This figure from
      <a href="https://arxiv.org/abs/2203.03466">Yang et al.¬†(2022)</a>
      shows both width transfer of optimal learning rate (loss minima
      fall on a vertical line) and ‚Äúwider is better‚Äù (larger widths
      reach lower loss).
      </div>
      </div>
      <p>It‚Äôs very much an open question whether anything like this can
      be shown generally true in any setting where all layers are
      trained.</p>
      <div class="question-box" id="oq-3-4">
      <p><strong>Open Question 3.4: Is wider better?</strong> Can it be
      shown that, when all hyperparameters are all optimally tuned, a
      wider MLP performs better on average on arbitrary tasks (perhaps
      under some reasonable assumptions on task structure)?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <p>Answering this question would be quite impactful: it seems
      almost within reach, and it would open up a new type of question
      for analytical theory.</p>
      <p>It‚Äôd also be interesting to know if there are counterexamples,
      even if they‚Äôre pretty handcrafted or unrealistic. Looking for
      counterexamples is probably an easier place to start than trying
      to prove the general theorem and might tell us if we‚Äôre barking up
      the wrong tree.</p>
      <div class="question-box" id="oq-3-5">
      <p><strong>Open Question 3.5: ‚ÄúWider is better‚Äù
      counterexample.</strong> Is there a nontrivial example of a task
      for which a wider network does not perform better, even when all
      other hyperparameters are optimally tuned?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <h3 id="depth">Depth</h3>
      <p>Network depth is also amenable to a treatment similar to that
      used to derive <span class="math inline">\(\mu\)</span>P. Even
      before <span class="math inline">\(\mu\)</span>P, though, we knew
      that large depth was a more finicky limit than large width.</p>
      <ul>
      <li><a href="https://arxiv.org/abs/1611.01232?">[Schoenholz et
      al.¬†(2016)]</a> found that infinite depth MLPs are
      ill-conditioned, even at infinite width. There are two different
      ways they can be ill-conditioned (‚Äùordered‚Äù and ‚Äúchaotic‚Äù), but
      both are bad for training.
      <ul>
      <li>It‚Äôs worth reading enough of this one that you can understand
      Figure 1 and could reproduce it if you had to. It‚Äôs important to
      get some intuitive feel for why infinite depth leads to
      ill-conditioned representations and gradients.</li>
      </ul></li>
      <li><a href="https://arxiv.org/abs/1909.05989">[Hanin and Nica
      (2020)]</a> found that finite-width corrections to the NTK of a
      randomly-initialized ReLU net scale like <span
      class="math inline">\(\exp(\text{depth/width})\)</span>. That is,
      you need <span class="math inline">\(\text{depth} \ll
      \text{width}\)</span> to be in a deterministic ‚Äúlarge width‚Äù
      regime. Otherwise, fluctuations due to the random init really
      matter.
      <ul>
      <li>You don‚Äôt need to follow the detailed calculation here, but
      it‚Äôs worth getting some intuition for why it‚Äôs true. Even a simple
      random matrix calculation works well enough to show this: take a
      product of <span class="math inline">\(L\)</span> square random
      matrices of large dimension <span class="math inline">\([n \times
      n]\)</span>, and you‚Äôll find that the singular spectrum of the
      product is close to deterministic when <span
      class="math inline">\(n \gg L\)</span> and fluctuates otherwise.<a
      href="#fn5" class="footnote-ref" id="fnref5"
      role="doc-noteref"><sup>5</sup></a></li>
      </ul></li>
      </ul>
      <p>The moral of the above story is that you generally want to take
      width to infinity before you take depth to infinity, and you
      probably shouldn‚Äôt take depth to infinity if your model is a naive
      feedforward MLP.</p>
      <p>The proper way to take depth to infinity involves a <em>ResNet
      formulation</em> with <em>downweighted layers.</em> Take a deep
      ResNet with <span class="math inline">\(L \gg 1\)</span> layers.
      The activations will explode at init as you forward propagate
      through many layers unless you multiply each layer by a small
      factor so the total accumulated change remains order one (or, more
      properly, the same order as you‚Äôd get from one regular ResNet
      layer).</p>
      <ul>
      <li><a href="https://arxiv.org/abs/2309.16620">[Bordelon et
      al.¬†(2023)]</a> and <a
      href="https://arxiv.org/abs/2310.02244">[Yang et al.¬†(2023)]</a>
      show how, by applying an attenuating factor to each residual
      branch, you can get a well-behaved limit. The scalings they find
      allow depthwise hyperparameter transfer.
      <ul>
      <li>These two papers offer subtly different prescriptions for the
      layerwise learning rates and attenuation factors. Both result
      per-layer feature updates that scale as <span
      class="math inline">\(L^{-1}\)</span>.</li>
      <li>The core ideas here are important if you want to study large
      or infinite depth. Large depth will ultimately be important ‚Äî it
      seems likely that the final theory of deep learning will take
      place at infinite depth ‚Äî but depth can be skipped on a first
      pass.</li>
      </ul></li>
      </ul>
      <div class="question-box" id="oq-3-6">
      <p><strong>Open Question 3.6: Is deeper better?</strong> Can it be
      shown that, when all hyperparameters are all optimally tuned, a
      <em>deeper</em> MLP performs better on average on arbitrary tasks
      (perhaps under some reasonable assumptions on task structure)?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <div class="question-box" id="oq-3-7">
      <p><strong>Open Question 3.7: ‚ÄúDeeper is better‚Äù
      counterexample.</strong> Is there a nontrivial example of a task
      for which a <em>deeper</em> network does not perform better, even
      when all other hyperparameters are optimally tuned?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <h3 id="batch-size">Batch size</h3>
      <p>Batch size is a tricky hyperparameter. At present, we have no
      unified theory like <span class="math inline">\(\mu\)</span>P. The
      best we have are empirical rules of thumb. A larger batch size
      gives you a better estimate of the true population gradient, which
      is generally desirable. In general, the larger the batch size, the
      fewer steps you need to reach a particular loss level, but the
      more expensive a single batch is to compute. The optimal value
      will fall somewhere in between one and infinity, and will depend
      on the task, the learning rate, and potentially your compute
      budget.</p>
      <ul>
      <li><a href="https://arxiv.org/abs/1812.06162">[McCandlish et
      al.¬†(2018)]</a> find an empirical rule relating the covariance of
      the gradient over samples to the compute-optimal batch size. Their
      prescription is essentially to increase batch size until the
      gradient noise is roughly the same size as the true gradient you
      want to estimate. This seems to work well across many tasks.
      <ul>
      <li>For theorists, the key material is probably Section 2.2. They
      use a simple calculation on a quadratic loss to estimate the
      effect of batch noise, and then simplify it with the (major)
      assumption that the loss Hessian is proportional to the identity.
      Despite these dramatic simplifications, they get out a
      prescription for batch size that seems to work empirically, which
      is a sign that there‚Äôs probably something simple going on that we
      could understand.</li>
      </ul></li>
      </ul>
      <div class="question-box" id="oq-3-8">
      <p><strong>Open Question 3.8: Explaining compute-optimal batch
      sizes.</strong> Why does the batch size prescription of <a
      href="https://arxiv.org/abs/1812.06162">[McCandlish et
      al.¬†(2018)]</a> based on an assumption of isotropic, quadratic
      loss nonetheless predict compute-optimal batch size in a variety
      of realistic tasks?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <p>We can study the effect of batch size in linear models, but of
      course linear models are not neural networks, and it‚Äôs as yet
      unclear how to transfer insights.</p>
      <ul>
      <li><a href="https://arxiv.org/abs/2102.04396">[Paquette et
      al.¬†(2021)]</a> and <a
      href="https://arxiv.org/abs/2106.02713">[Bordelon and Pehlevan
      (2022)]</a> study the optimization of SGD on linear models,
      including studies of the effective batch size.</li>
      </ul>
      <p>Understanding batch size in practical networks is a good open
      task for theorists. Until we have that understanding, when doing
      rigorous science, it‚Äôs best to either train with a small enough
      learning rate that the batch size doesn‚Äôt matter, or else tread
      with caution.</p>
      <h3 id="transformer-specific-hyperparameters">Transformer-specific
      hyperparameters</h3>
      <p>Transformers have a large number of architectural
      hyperparameters specific to their architectures. Basically every
      large model these days is or incorporates a transformer, so it‚Äôs
      useful to study these hyperparameters. Of all the categories of
      hyperparameter, this is the most important for major industry
      labs, so they very likely know quite a bit that‚Äôs not public
      knowledge, at least in the form of empirical rules of thumb. Here
      are some highlights of what is currently publicly known.</p>
      <ul>
      <li><a href="https://arxiv.org/abs/2405.15712">[Bordelon et
      al.¬†(2024)]</a> studied infinite-width limits in transformers. You
      can increase width either by increasing the number of attention
      heads or the size of each head, and these yield different
      infinite-width limits.</li>
      <li><a href="https://arxiv.org/abs/2203.15556">[Hoffman et
      al.¬†(2022)]</a> empirically study the tradeoff between dataset
      size, model size, and compute iterations. They find the ‚ÄúChincilla
      rule‚Äù for LLM pretraining: the optimal scaling takes <span
      class="math inline">\(B \cdot T / \text{[num. params.]} \approx
      20\)</span>. That is, the batch size times the number of steps ‚Äî
      equal to the size of the dataset in online training ‚Äî is
      proportional to the model size, with a proportionality constant of
      about <span class="math inline">\(20\)</span>.
      <ul>
      <li>This is an extremely suggestive empirical finding, since it
      admits the interpretation that each parameter is ‚Äústoring the
      information‚Äù of <span class="math inline">\(20\)</span> samples.
      The number <span class="math inline">\(20\)</span> isn‚Äôt as
      important as the fact that these quantities are proportional. It
      seems likely that explaining this would be a major step in the
      quest to understand deep learning.</li>
      </ul></li>
      <li>Within a transformer model family, it is common to scale width
      (i.e.¬†the hidden dimension of token embeddings) up proportional to
      depth. Compare, for example, GPT-3 6.7B and 175B in <a
      href="https://arxiv.org/abs/2005.14165">[Brown et al.¬†(2020)]</a>.
      It‚Äôs not known why this is optimal.</li>
      </ul>
      <div class="question-box" id="oq-3-9">
      <p><strong>Open Question 3.9: Why tokens <span
      class="math inline">\(\propto\)</span> parameters in
      LLMs?</strong> Why is the compute-optimal prescription for LLMs a
      fixed number of tokens per parameter? A good place to start may be
      a study of random feature regression, in which the eigenframework
      of e.g.¬†<a href="https://arxiv.org/abs/2311.14646">[Simon et
      al.¬†(2024)]</a> will correctly predict that the number of
      parameters and number of samples should scale proportionally for
      compute-optimal performance. Can a more general argument be
      extracted from consideration of this simple model? The correctness
      of a proposed explanation should be confirmed by making some new
      prediction that can be tested with transformers, such as how
      changing the task difficulty affects the optimal
      tokens-to-parameters ratio.</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <div class="question-box" id="oq-3-10">
      <p><strong>Open Question 3.10: Why depth <span
      class="math inline">\(\propto\)</span> width in LLMs?</strong>
      Why, judging by publicly-reported LLM architecture specs, is it
      seemingly optimal to scale transformer depth proportional to
      width?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <div class="question-box" id="oq-3-11">
      <p><strong>Open Question 3.11: Hyperparameter scaling for
      MoEs.</strong> What hyperparameter scaling prescriptions apply to
      mixtures of experts? Can the central arguments of <span
      class="math inline">\(\mu\)</span>P be imported and used to obtain
      initialization scales and learning rates that give rich training
      at infinite width?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <h3 id="activation-function">Activation function</h3>
      <p>This was probably the first seriously debated hyperparameter.
      It‚Äôs pretty easy to come up with new activation functions, and so
      there are many: classics like tanh and the sigmoid gave way to
      ReLU, and now we have variants including ELU, GELU, SELU, SwiGLU.
      Practically speaking, the upshot is that ReLU works pretty well,
      and you don‚Äôt need to look far from it.</p>
      <p>Why ReLU basically just works for everything remains poorly
      understood. A pretty good starting point is the deep information
      propagation analysis of <a
      href="https://arxiv.org/abs/1611.01232?">[Schoenholz et
      al.¬†(2016)]</a>. Sitting with this for some time, you‚Äôll find that
      ReLU has some desirable stability properties: it‚Äôs easy to
      initialize at the edge of chaos, and ReLU‚Äôs homogeneity means that
      the activation function ‚Äúlooks interesting‚Äù no matter the scale of
      the input. Nonetheless, despite a lot of effort in the late 2010s,
      people have basically stopped asking why ReLU is so good. We‚Äôll
      list it here as an open question.</p>
      <div class="question-box" id="oq-3-12">
      <p><strong>Open Question 3.12: Why ReLU?</strong> Why is ReLU close to
      the optimal activation function for most deep learning
      applications? A scientific answer to this question should include
      calculations and convincing experiments that make the case.</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <h3 id="new-frontiers">New frontiers</h3>
      <p>There‚Äôs a lot we don‚Äôt know. There are plenty of
      hyperparameters left to study (including almost all the
      architectural and dataset hyperparameters), but it‚Äôs not yet clear
      (at least to us) where to start. Here are a few leads on problems
      that seem in reach.</p>
      <p>Norm layers are quite mysterious. Nobody really knows how to do
      theory that treats norm layers in what feels like the right way to
      understand their use in practice. This seems fairly doable. It‚Äôs
      easy to make wrong assumptions about norm layers, so a study here
      should probably start with empirics.</p>
      <div class="question-box" id="oq-3-13">
      <p><strong>Open Question 3.13: What‚Äôs even going on with norm
      layers?</strong> What scaling relationships apply to norm layers
      embedded within deep neural networks? We‚Äôre interested here in
      both hyperparameter scaling prescriptions like <span
      class="math inline">\(\mu\)</span>P and empirical scaling
      relationships which relate, say, the number or strength of norm
      layers to statistics of model weights, representations, or
      performance.</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <div class="question-box" id="oq-3-14">
      <p><strong>Open Question 3.14: Do we really need norm layers?</strong>
      There is a feeling among practitioners and theorists alike that
      norm layers are somewhat unnatural. Can their effect on
      forward-propagation and training be characterized well enough that
      they can be replaced by something more mathematically elegant?
      Even if this does not yield better performance, it would be a step
      towards an interpretable science of large models.</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <p>Other optimizers have lots of fiddly bits on their
      hyperparameters. Weight decay and momentum can usually be treated
      under <span class="math inline">\(\mu\)</span>P in the same breath
      as learning rate, though it gets talked about less. We haven‚Äôt yet
      seen a full and convincing treatment of Adam‚Äôs hyperparameters,
      though. Given Adam‚Äôs wide use, that seems worth doing.</p>
      <div class="question-box" id="oq-3-15">
      <p><strong>Open Question 3.15: What‚Äôs even going on with Adam?</strong>
      What scaling relationships apply to Adam‚Äôs <span
      class="math inline">\(\beta\)</span> or <span
      class="math inline">\(\epsilon\)</span> hyperparameters?</p>
      </div><div class="oq-see-all"><a href="../open-questions.html">See all open questions</a></div>
      <section id="footnotes"
      class="footnotes footnotes-end-of-document" role="doc-endnotes">
      <hr />
      <ol>
      <li id="fn1"><p>The ones listed here are just the
      commonly-enumerated optimization hyperparameters! The more you
      think about it, the more implicit hyperparameters you‚Äôll notice.
      For example, different layers might want different learning rates
      for optimal training. It‚Äôs common to use a learning rate
      <em>schedule</em> ‚Äî changing the learning rate with time as some
      function <span class="math inline">\(\eta(t)\)</span> ‚Äî in which
      case the learning rate is a <em>functional degree of freedom,</em>
      not just a single constant. It‚Äôs commonly discussed (though less
      commonly done) to use <em>non-</em>Gaussian init, in which case
      you have a distributional degree of freedom. Noticing these
      implicit hyperparameters makes it clear that the default choices ‚Äî
      constant learning rate, say, or Gaussian init ‚Äî actually
      <em>are</em> choices. Sometimes a default choice turns out not to
      be the best or simplest one, and so it behooves the theorist to
      notice these implicit choices. For example, using different
      learning rates for different layers is an essential part of <span
      class="math inline">\(\mu\)</span>P.<a href="#fnref1"
      class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn2"><p>You should usually demand that someone shows you
      at least a factor-of-ten change in a hyperparameter before you
      believe you‚Äôre effectively in a limiting regime. Sometimes the
      change can be slow, and a factor of two showing small change can
      be deceptive.<a href="#fnref2" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn3"><p>Finding hyperparameter scaling relationships is
      very similar to <a
      href="https://en.wikipedia.org/wiki/Nondimensionalization">nondimensionalization</a>
      in physics! If you‚Äôre a physicist, quantities like effective
      training time <span class="math inline">\(\tau\)</span> should
      feel like nondimensionalized variables.<a href="#fnref3"
      class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn4"><p>Our sense is that the difficulty of analytically
      studying most systems in deep learning scales roughly
      exponentially with the number of hyperparameters you have to treat
      simultaneously. It‚Äôs a <em>really</em> good idea to get that
      number low.<a href="#fnref4" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn5"><p>If anyone wants to do this experiment and make
      some plots, we‚Äôll add them here :)<a href="#fnref5"
      class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      </ol>
      </section>
    </main>
  </article>

  <hr><div class="sequence-toc"><h3>A Quickstart Guide to Learning Mechanics</h3><ol><li><a href="../quickstart/introduction.html">Introduction: what do you want to understand?</a></li><li><a href="../quickstart/hidden-representations.html">...the average size of hidden representations?</a></li><li><strong>...hyperparameter selection (and why should theorists care)?</strong></li><li><a href="../quickstart/optimization.html">üöß ...the convergence and stability of optimization?</a></li><li><a href="../quickstart/feature-learning.html">üöß ...feature learning and the final network weights?</a></li><li><a href="../quickstart/generalization.html">üöß ...generalization?</a></li><li><a href="../quickstart/sparsity.html">üöß ...neuron-level sparsity?</a></li><li><a href="../quickstart/data-structure.html">üöß ...the structure in the data?</a></li><li><a href="../quickstart/conclusion.html">üöß Places to make a difference</a></li></ol></div><div class="back-to-top"><a href="#top"><i class="fas fa-arrow-circle-up"></i></a></div>

  <div class="comments">
    <h2>Comments</h2>
    <script>
      (function() {
        var theme = localStorage.getItem('theme') || 'light';
        var s = document.createElement('script');
        s.src = 'https://giscus.app/client.js';
        s.setAttribute('data-repo', 'learningmechanics/learningmechanics.github.io');
        s.setAttribute('data-repo-id', 'R_kgDOO_nSsA');
        s.setAttribute('data-category', 'Blog Comments');
        s.setAttribute('data-category-id', 'DIC_kwDOO_nSsM4Cualh');
        s.setAttribute('data-mapping', 'pathname');
        s.setAttribute('data-strict', '0');
        s.setAttribute('data-reactions-enabled', '1');
        s.setAttribute('data-emit-metadata', '0');
        s.setAttribute('data-input-position', 'bottom');
        s.setAttribute('data-theme', theme);
        s.setAttribute('data-lang', 'en');
        s.setAttribute('crossorigin', 'anonymous');
        s.async = true;
        document.currentScript.parentNode.appendChild(s);
      })();
    </script>
  </div>


  <script>
    // Font switcher
    const fonts = [
      { name: 'ET Book', family: "'et-book', 'ETBookOT', 'ET Book', Georgia, 'Times New Roman', serif" },
      { name: 'Gill Sans', family: "'Gill Sans', 'Gill Sans MT', Calibri, sans-serif" },
      { name: 'Georgia', family: "Georgia, 'Times New Roman', serif" },
      { name: 'Palatino', family: "Palatino, 'Palatino Linotype', 'Book Antiqua', serif" },
      { name: 'Garamond', family: "Garamond, 'Apple Garamond', 'Baskerville', serif" },
      { name: 'Baskerville', family: "Baskerville, 'Baskerville Old Face', 'Hoefler Text', Garamond, serif" },
      { name: 'Times', family: "'Times New Roman', Times, serif" },
      { name: 'Helvetica', family: "Helvetica, Arial, sans-serif" },
      { name: 'Comic Sans', family: "'Comic Sans MS', 'Comic Sans', cursive" },
      { name: 'Wingdings', family: "Wingdings, 'Zapf Dingbats', serif" }
    ];

    function updateFontButton(fontName) {
      const button = document.querySelector('.font-toggle');
      button.textContent = `üî§ font:  (click to change)`;
    }

    function cycleFont() {
      const currentFont = localStorage.getItem('font') || 'ET Book';
      const currentIndex = fonts.findIndex(f => f.name === currentFont);
      const nextIndex = (currentIndex + 1) % fonts.length;
      const nextFont = fonts[nextIndex];

      document.body.style.fontFamily = nextFont.family;
      localStorage.setItem('font', nextFont.name);
      updateFontButton(nextFont.name);
    }

    function toggleTheme() {
      const currentTheme = document.documentElement.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';

      document.documentElement.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);

      // Update button icon
      const icon = document.querySelector('.theme-toggle i');
      icon.className = newTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

      // Update Giscus theme
      const giscusFrame = document.querySelector('iframe.giscus-frame');
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage({
          giscus: { setConfig: { theme: newTheme } }
        }, 'https://giscus.app');
      }
    }

    // Initialize theme and font on page load
    document.addEventListener('DOMContentLoaded', function() {
      // Theme
      const savedTheme = localStorage.getItem('theme') || 'light';
      document.documentElement.setAttribute('data-theme', savedTheme);

      const icon = document.querySelector('.theme-toggle i');
      icon.className = savedTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

      // Font
      const savedFont = localStorage.getItem('font') || 'ET Book';
      const font = fonts.find(f => f.name === savedFont) || fonts[0];
      document.body.style.fontFamily = font.family;
      updateFontButton(font.name);
    });
  </script>
</body>
</html> 