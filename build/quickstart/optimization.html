<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Quickstart Guide: The dynamics of
optimization - Learning Mechanics</title>
  
  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script defer src="../static/math-render.js"></script>
  
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  
  <!-- Our minimal styles -->
  <link rel="stylesheet" href="../static/style.css">
  
  <!-- Meta tags -->
  <meta name="description" content="Major lines of theoretical research
into the optimization dynamics of neural networks, from classical
convergence theory to the edge of stability.">
  <meta name="author" content="The Learning Mechanics Team">
  <meta name="date" content="2025-09-01">
</head>
<body id="top">
  <a href="/" class="home-glyph"><i class="fas fa-home"></i></a>
  <button class="font-toggle" onclick="cycleFont()"></button>
  <button class="theme-toggle" onclick="toggleTheme()"><i class="fas fa-sun"></i></button>

  <article>
    <header>
            
            <h1>Quickstart Guide: The dynamics of optimization</h1>
            <div class="sequence-nav">
        Part 4 of <a href="../quickstart/introduction">A Quickstart
Guide to Learning
Mechanics</a> (<a href="../quickstart/hyperparameter-selection">prev</a> | <a href="../quickstart/feature-learning">next</a>)
      </div>
            <hr class="title-separator">
      <div class="metadata">
        <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">The Learning Mechanics Team</a>
        <br><time datetime="2025-09-01">2025-09-01</time>
      </div>
      <hr class="metadata-separator">
    </header>

    <main>
      <p>Neural network training is just a process of numerical
      optimization: first you define a loss (i.e.¬†cost) function you
      want to minimize, and then you push on all the neural network
      parameters to <em>make number go down.</em> You do this for a long
      time on a lot of data, and the loss goes down, and the network
      learns.</p>
      <p>This sounds really simple until you try it. It turns out there
      are lots of ways to make the number go down, and some of them work
      better than others, and some of them work very poorly even though
      they seem like bright ideas, and it‚Äôs all a big mess. It would
      sure be nice if we could simplify the picture and shed some light
      on the process of optimization, and understanding that process
      just might give us a new lens for understanding the final trained
      artifact we get out at training‚Äôs end.</p>
      <p>In this chapter, we‚Äôll walk through several major lines of
      theoretical research into the optimization dynamics of neural
      networks. We‚Äôll start with the classical perspective, which was
      concerned mostly with the <em>convergence</em> of optimization.
      This turned out to not be the right question to ask of deep
      learning, but that line of work still unearthed a few gems that
      continue to be useful.</p>
      <p>In recent times, we have learned that we should pay attention
      to more than just the value of the loss. ‚ÄúHow fast does it go
      down?‚Äù is essentially the only question you can ask about the
      loss, which isn‚Äôt all that interesting. We should also look at how
      the <strong><em>weights, hidden representations, and other
      high-dimensional statistics of the network</em></strong> change
      during training. It turns out most of the story is in these other
      quantities that then drive the loss dynamics.<a href="#fn1"
      class="footnote-ref" id="fnref1"
      role="doc-noteref"><sup>1</sup></a> To that end, we‚Äôll cover a few
      cases where notable optimization phenomena have been understood in
      parameter space.</p>
      <p>This will be another long chapter, so we‚Äôll start with a table
      of contents:</p>
      <div class="sequence-toc">
      <h3>
      Quickstart Guide: The dynamics of optimization
      </h3>
      <ol>
      <li>
      <a href="#the-classical-picture-optimization-as-distinct-from-generalization">The
      classical picture: optimization as distinct from
      generalization</a>
      </li>
      <li>
      <a href="#overparameterization-overfitting-and-the-slow-death-of-the-classical-perspective">Overparameterization,
      overfitting, and the slow death of the classical perspective</a>
      </li>
      <li>
      <a href="#the-inductive-bias-of-gradient-descent-and-the-ntk-picture">The
      inductive bias of gradient descent and the NTK picture</a>
      </li>
      <li>
      <a href="#deep-linear-nets-a-solvable-case-of-dynamics-in-weight-space">Deep
      linear nets: a solvable case of dynamics in weight space</a>
      </li>
      <li>
      <a href="#progressive-sharpening-and-the-edge-of-stability">Progressive
      sharpening and the edge of stability</a>
      </li>
      <li>
      <a href="#nondimensionalization-and-scale-invariance">Nondimensionalization
      and scale-invariance</a>
      </li>
      <li>
      <a href="#why-do-some-optimizers-work-better-than-others">Why do
      some optimizers work better than others?</a>
      </li>
      </ol>
      </div>
      <h3
      id="the-classical-picture-optimization-as-distinct-from-generalization">The
      classical picture: optimization as distinct from
      generalization</h3>
      <ul>
      <li>Jeremy‚Äôs picture; separation of concerns</li>
      <li>Classical results about optimization:
      <ul>
      <li>Descent lemma, how beautiful</li>
      <li>Quadratic loss surfaces</li>
      <li>Classical/convex optimization theory</li>
      </ul></li>
      </ul>
      <h3
      id="overparameterization-overfitting-and-the-slow-death-of-the-classical-perspective">Overparameterization,
      overfitting, and the slow death of the classical perspective</h3>
      <ul>
      <li>A bunch of things wrong with the classical picture:
      <ul>
      <li>E.g. the same training loss can generalize very differently
      <ul>
      <li>So clearly there‚Äôs ‚Äúsomething to‚Äù the training dynamics that‚Äôs
      not reflected in the loss</li>
      </ul></li>
      <li>Intuition that with many params you can always find a descent
      direction; no bad local minima</li>
      </ul></li>
      </ul>
      <h3
      id="the-inductive-bias-of-gradient-descent-and-the-ntk-picture">The
      inductive bias of gradient descent and the NTK picture</h3>
      <ul>
      <li>Inductive bias story: GD ‚Äúchooses‚Äù from a manifold of local
      minima and somehow ‚Äúprefers‚Äù one
      <ul>
      <li>Solvable cases where this is true</li>
      <li>Not that it‚Äôs wrong, just that it‚Äôs hard to really make this
      rigorous</li>
      <li>Still is a folk belief that GD is kinda like L2-min, tho‚Ä¶ how
      true? Open Q.</li>
      </ul></li>
      <li>NTK: a solvable model of wide neural nets
      <ul>
      <li>Kinda killed the question of overparameterization</li>
      <li>Already discussed in previous two sections; intersects with
      those stories here</li>
      </ul></li>
      </ul>
      <h3
      id="deep-linear-nets-a-solvable-case-of-dynamics-in-weight-space">Deep
      linear nets: a solvable case of dynamics in weight space</h3>
      <ul>
      <li>Arguably the first case people really solved dynamics of a
      deep model in weight space proper
      <ul>
      <li>Be sure to give citations to earlier works that do this, too,
      incl that Japanese guy</li>
      </ul></li>
      <li>Discuss what happens, the phenomenology, link to widget
      if/when we have it</li>
      <li>Greedy low-rank dynamics: discuss s2s, AGF, etc.</li>
      <li>Does this look like real nets? If so, how so? Jury‚Äôs still
      out.</li>
      </ul>
      <div class="question-box" id="oq-4-1">
      <p><strong>Open Question 4.1: Deep linear net dynamics in real
      networks.</strong> To what extent do deep linear network dynamics
      (e.g.¬†greedy low-rank progression) carry over to nonlinear
      networks trained in practice?</p>
      </div><div class="oq-see-all"><a href="../openquestions">See all open questions</a></div>
      <h3
      id="progressive-sharpening-and-the-edge-of-stability">Progressive
      sharpening and the edge of stability</h3>
      <ul>
      <li>Classical intuition: sharpness stability threshold</li>
      <li>Modern observation: nets actually adapt to approach that
      threshold
      <ul>
      <li>Bluntening: catapult effect</li>
      <li>Sharpening: progressive sharpening</li>
      </ul></li>
      <li>And then you‚Äôre at the EoS. And there‚Äôs actually a great deal
      you can say there
      <ul>
      <li>Jeremy‚Äôs papers, Alex‚Äôs papers</li>
      </ul></li>
      </ul>
      <h3
      id="nondimensionalization-and-scale-invariance">Nondimensionalization
      and scale-invariance</h3>
      <ul>
      <li>This is basically the HP story from the previous chapter</li>
      <li>Discuss how it came about + what it means, how it ties into
      this chapter</li>
      <li>Cite Nikhil‚Äôs recent paper here</li>
      </ul>
      <h3 id="why-do-some-optimizers-work-better-than-others">Why do
      some optimizers work better than others?</h3>
      <ul>
      <li>‚ÄúAdam works by being block-adaptive w its lr‚Äù</li>
      <li>Muon??? Kiiiinda comes from the spectral FL perspective?</li>
      </ul>
      <section id="footnotes"
      class="footnotes footnotes-end-of-document" role="doc-endnotes">
      <hr />
      <ol>
      <li id="fn1"><p>To make this point in analogy form: trying to
      understand optimization by just studying the loss is like trying
      to understand a country‚Äôs history, culture and politics from just
      its GDP over time.<a href="#fnref1" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      </ol>
      </section>
    </main>
  </article>

  <hr><div class="sequence-toc"><h3>A Quickstart Guide to Learning Mechanics</h3><ol><li><a href="../quickstart/introduction">Introduction: asking a specific question</a></li><li><a href="../quickstart/hidden-representations">The average size of hidden representations</a></li><li><a href="../quickstart/hyperparameter-selection">Hyperparameter selection (and why theorists should care)</a></li><li><strong>The dynamics of optimization</strong></li><li><a href="../quickstart/feature-learning">üöß Feature learning and the final network weights</a></li><li><a href="../quickstart/generalization">üöß Generalization</a></li><li><a href="../quickstart/sparsity">üöß Neuron-level sparsity</a></li><li><a href="../quickstart/data-structure">üöß The structure in the data</a></li><li><a href="../quickstart/conclusion">üöß Places to make a difference</a></li></ol></div><div class="back-to-top"><a href="#top"><i class="fas fa-arrow-circle-up"></i></a></div>

  <div class="comments">
    <h2>Comments</h2>
    <script>
      (function() {
        var theme = localStorage.getItem('theme') || 'light';
        var s = document.createElement('script');
        s.src = 'https://giscus.app/client.js';
        s.setAttribute('data-repo', 'learningmechanics/learningmechanics.github.io');
        s.setAttribute('data-repo-id', 'R_kgDOO_nSsA');
        s.setAttribute('data-category', 'Blog Comments');
        s.setAttribute('data-category-id', 'DIC_kwDOO_nSsM4Cualh');
        s.setAttribute('data-mapping', 'pathname');
        s.setAttribute('data-strict', '0');
        s.setAttribute('data-reactions-enabled', '1');
        s.setAttribute('data-emit-metadata', '0');
        s.setAttribute('data-input-position', 'bottom');
        s.setAttribute('data-theme', theme);
        s.setAttribute('data-lang', 'en');
        s.setAttribute('crossorigin', 'anonymous');
        s.async = true;
        document.currentScript.parentNode.appendChild(s);
      })();
    </script>
  </div>


  <script>
    // Font switcher
    const fonts = [
      { name: 'ET Book', family: "'et-book', 'ETBookOT', 'ET Book', Georgia, 'Times New Roman', serif" },
      { name: 'Gill Sans', family: "'Gill Sans', 'Gill Sans MT', Calibri, sans-serif" },
      { name: 'Georgia', family: "Georgia, 'Times New Roman', serif" },
      { name: 'Palatino', family: "Palatino, 'Palatino Linotype', 'Book Antiqua', serif" },
      { name: 'Garamond', family: "Garamond, 'Apple Garamond', 'Baskerville', serif" },
      { name: 'Baskerville', family: "Baskerville, 'Baskerville Old Face', 'Hoefler Text', Garamond, serif" },
      { name: 'Times', family: "'Times New Roman', Times, serif" },
      { name: 'Helvetica', family: "Helvetica, Arial, sans-serif" },
      { name: 'Comic Sans', family: "'Comic Sans MS', 'Comic Sans', cursive" },
      { name: 'Wingdings', family: "Wingdings, 'Zapf Dingbats', serif" }
    ];

    function updateFontButton(fontName) {
      const button = document.querySelector('.font-toggle');
      button.textContent = `üî§ font:  (click to change)`;
    }

    function cycleFont() {
      const currentFont = localStorage.getItem('font') || 'ET Book';
      const currentIndex = fonts.findIndex(f => f.name === currentFont);
      const nextIndex = (currentIndex + 1) % fonts.length;
      const nextFont = fonts[nextIndex];

      document.body.style.fontFamily = nextFont.family;
      localStorage.setItem('font', nextFont.name);
      updateFontButton(nextFont.name);
    }

    function toggleTheme() {
      const currentTheme = document.documentElement.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';

      document.documentElement.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);

      // Update button icon
      const icon = document.querySelector('.theme-toggle i');
      icon.className = newTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

      // Update Giscus theme
      const giscusFrame = document.querySelector('iframe.giscus-frame');
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage({
          giscus: { setConfig: { theme: newTheme } }
        }, 'https://giscus.app');
      }
    }

    // Initialize theme and font on page load
    document.addEventListener('DOMContentLoaded', function() {
      // Theme
      const savedTheme = localStorage.getItem('theme') || 'light';
      document.documentElement.setAttribute('data-theme', savedTheme);

      const icon = document.querySelector('.theme-toggle i');
      icon.className = savedTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

      // Font
      const savedFont = localStorage.getItem('font') || 'ET Book';
      const font = fonts.find(f => f.name === savedFont) || fonts[0];
      document.body.style.fontFamily = font.family;
      updateFontButton(font.name);
    });
  </script>
</body>
</html> 