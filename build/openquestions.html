<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Compendium of Open Questions ‚Äî Learning Mechanics</title>

  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script defer src="static/math-render.js"></script>

  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

  <!-- Our minimal styles -->
  <link rel="stylesheet" href="static/style.css">

  <meta name="description" content="Open questions in the science of deep learning.">
</head>
<body>
  <a href="/" class="home-glyph"><i class="fas fa-home"></i></a>
  <button class="font-toggle" onclick="cycleFont()"></button>
  <button class="theme-toggle" onclick="toggleTheme()"><i class="fas fa-sun"></i></button>

  <header>
    <h1>Compendium of Open Questions</h1>
    <hr class="header-separator">
    <div class="site-description">
      <p>If you want to help build learning mechanics, pick an interesting open question and get curious about it! On this page you'll find a collection of open questions we think are great places to start poking around. Some of them are broad and fertile directions, and some are smaller questions that might be resolved in a single paper. If they're listed here, we think they're fertile ground for an individual or a small team to start digging.</p>
      <br>
      <p>If and when we see papers that make significant progress towards any of these open questions, we'll add pointers to those works to this page.</p>
    </div>
    <hr class="header-separator">
  </header>

  <main>
    <div class="open-questions-list">

      <div class="sequence-box oq-group" id="big-open-directions">
        <div class="sequence-title">Big open directions</div>
        <p class="oq-placeholder"><em>Coming soon ‚Äî these will be drawn from our position paper.</em></p>
      </div>

      
    <div class="sequence-box oq-group" id="quickstart-questions">
      <div class="sequence-title">Open questions from <a href="quickstart/introduction"><em>A Quickstart Guide to Learning Mechanics</em></a></div>
      <div class="question-box"><p><strong>Open Question 2.1: Convergence of wide <span
      class="math inline">\(\mu\)</span>P networks.</strong> Under what
      conditions does a network in the infinite-width <span
      class="math inline">\(\mu\)</span>P limit converge when optimized
      with gradient descent?</p></div>
      <div class="oq-see-all"><a href="quickstart/hidden-representations#oq-2-1">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 2.2: Framework for studying feature learning
      at large width.</strong> is there a simple, computationally
      tractable calculational framework ‚Äî potentially making realistic
      simplifying assumptions ‚Äî that allows us to quantitatively study
      feature evolution of a general class of neural network in the rich
      regime and which requires tracking less information than the DMFT
      framework of <a href="https://arxiv.org/abs/2205.09653">[Bordelon
      and Pehlevan (2022)]</a>?</p></div>
      <div class="oq-see-all"><a href="quickstart/hidden-representations#oq-2-2">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 2.3: Framework for studying feature learning
      at large width and depth.</strong> is there a simple,
      computationally tractable calculational framework ‚Äî potentially
      making realistic simplifying assumptions ‚Äî that allows us to
      quantitatively study the feature evolution of an
      infinite-<em>depth</em> network in the rich regime?</p></div>
      <div class="oq-see-all"><a href="quickstart/hidden-representations#oq-2-3">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.1: Optimal hyperparameters for a simple
      nonlinear model.</strong> In a simple but nontrivial model ‚Äî say,
      a linear network of infinite width but finite depth, trained with
      population gradient descent ‚Äî what are the optimal choices for the
      layerwise init scales and learning rates ‚Äì not just the width
      scalings but also the constant prefactors? Are they the same or
      different between layers? Do empirics reveal discernible patterns
      that theory might aim to explain?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-1">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.2: Scaling relationships for learning rate
      schedules.</strong> What scaling rules or relationships apply to
      learning rate schedules? What nondimensionalized quantities
      emerge? Can we ‚Äúpost-dict‚Äù properties of common learning rate
      schedules used in practice?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-2">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.3: Is richer better?</strong> <a
      href="https://arxiv.org/abs/2410.04642">[Atanasov et
      al.¬†(2024)]</a> find that, in online training, networks with
      larger richness parameter <span
      class="math inline">\(\gamma\)</span> generalize better (so long
      as they‚Äôre given enough training time to escape the initial
      plateau). Is this generally true? Why?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-3">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.4: Is wider better?</strong> Can it be
      shown that, when all hyperparameters are all optimally tuned, a
      wider MLP performs better on average on arbitrary tasks (perhaps
      under some reasonable assumptions on task structure)?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-4">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.5: ‚ÄúWider is better‚Äù
      counterexample.</strong> Is there a nontrivial example of a task
      for which a wider network does not perform better, even when all
      other hyperparameters are optimally tuned?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-5">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.6: Is deeper better?</strong> Can it be
      shown that, when all hyperparameters are all optimally tuned, a
      <em>deeper</em> MLP performs better on average on arbitrary tasks
      (perhaps under some reasonable assumptions on task structure)?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-6">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.7: ‚ÄúDeeper is better‚Äù
      counterexample.</strong> Is there a nontrivial example of a task
      for which a <em>deeper</em> network does not perform better, even
      when all other hyperparameters are optimally tuned?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-7">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.8: Explaining compute-optimal batch
      sizes.</strong> Why does the batch size prescription of <a
      href="https://arxiv.org/abs/1812.06162">[McCandlish et
      al.¬†(2018)]</a> based on an assumption of isotropic, quadratic
      loss nonetheless predict compute-optimal batch size in a variety
      of realistic tasks?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-8">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.9: Why tokens <span
      class="math inline">\(\propto\)</span> parameters in
      LLMs?</strong> Why is the compute-optimal prescription for LLMs a
      fixed number of tokens per parameter? A good place to start may be
      a study of random feature regression, in which the eigenframework
      of e.g.¬†<a href="https://arxiv.org/abs/2311.14646">[Simon et
      al.¬†(2024)]</a> will correctly predict that the number of
      parameters and number of samples should scale proportionally for
      compute-optimal performance. Can a more general argument be
      extracted from consideration of this simple model? The correctness
      of a proposed explanation should be confirmed by making some new
      prediction that can be tested with transformers, such as how
      changing the task difficulty affects the optimal
      tokens-to-parameters ratio.</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-9">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.10: Why depth <span
      class="math inline">\(\propto\)</span> width in LLMs?</strong>
      Why, judging by publicly-reported LLM architecture specs, is it
      seemingly optimal to scale transformer depth proportional to
      width?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-10">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.11: Hyperparameter scaling for
      MoEs.</strong> What hyperparameter scaling prescriptions apply to
      mixtures of experts? Can the central arguments of <span
      class="math inline">\(\mu\)</span>P be imported and used to obtain
      initialization scales and learning rates that give rich training
      at infinite width?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-11">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.12: Why ReLU?</strong> Why is ReLU close to
      the optimal activation function for most deep learning
      applications? A scientific answer to this question should include
      calculations and convincing experiments that make the case.</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-12">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.13: What‚Äôs even going on with norm
      layers?</strong> What scaling relationships apply to norm layers
      embedded within deep neural networks? We‚Äôre interested here in
      both hyperparameter scaling prescriptions like <span
      class="math inline">\(\mu\)</span>P and empirical scaling
      relationships which relate, say, the number or strength of norm
      layers to statistics of model weights, representations, or
      performance.</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-13">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.14: Do we really need norm layers?</strong>
      There is a feeling among practitioners and theorists alike that
      norm layers are somewhat unnatural. Can their effect on
      forward-propagation and training be characterized well enough that
      they can be replaced by something more mathematically elegant?
      Even if this does not yield better performance, it would be a step
      towards an interpretable science of large models.</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-14">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 3.15: What‚Äôs even going on with Adam?</strong>
      What scaling relationships apply to Adam‚Äôs <span
      class="math inline">\(\beta\)</span> or <span
      class="math inline">\(\epsilon\)</span> hyperparameters?</p></div>
      <div class="oq-see-all"><a href="quickstart/hyperparameter-selection#oq-3-15">See question in context</a></div>
      <div class="question-box"><p><strong>Open Question 4.1: Deep linear net dynamics in real
      networks.</strong> To what extent do deep linear network dynamics
      (e.g.¬†greedy low-rank progression) carry over to nonlinear
      networks trained in practice?</p></div>
      <div class="oq-see-all"><a href="quickstart/optimization#oq-4-1">See question in context</a></div>
    </div>
    </div>

    <footer></footer>
  </main>

  <script>
    const fonts = [
      { name: 'ET Book', family: "'et-book', 'ETBookOT', 'ET Book', Georgia, 'Times New Roman', serif" },
      { name: 'Gill Sans', family: "'Gill Sans', 'Gill Sans MT', Calibri, sans-serif" },
      { name: 'Georgia', family: "Georgia, 'Times New Roman', serif" },
      { name: 'Palatino', family: "Palatino, 'Palatino Linotype', 'Book Antiqua', serif" },
      { name: 'Garamond', family: "Garamond, 'Apple Garamond', 'Baskerville', serif" },
      { name: 'Baskerville', family: "Baskerville, 'Baskerville Old Face', 'Hoefler Text', Garamond, serif" },
      { name: 'Times', family: "'Times New Roman', Times, serif" },
      { name: 'Helvetica', family: "Helvetica, Arial, sans-serif" },
      { name: 'Comic Sans', family: "'Comic Sans MS', 'Comic Sans', cursive" },
      { name: 'Wingdings', family: "Wingdings, 'Zapf Dingbats', serif" }
    ];

    function updateFontButton(fontName) {
      const button = document.querySelector('.font-toggle');
      button.textContent = `üî§ font: ${fontName} (click to change)`;
    }

    function cycleFont() {
      const currentFont = localStorage.getItem('font') || 'ET Book';
      const currentIndex = fonts.findIndex(f => f.name === currentFont);
      const nextIndex = (currentIndex + 1) % fonts.length;
      const nextFont = fonts[nextIndex];
      document.body.style.fontFamily = nextFont.family;
      localStorage.setItem('font', nextFont.name);
      updateFontButton(nextFont.name);
    }

    function toggleTheme() {
      const currentTheme = document.documentElement.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
      document.documentElement.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);
      const icon = document.querySelector('.theme-toggle i');
      icon.className = newTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';
    }

    document.addEventListener('DOMContentLoaded', function() {
      const savedTheme = localStorage.getItem('theme') || 'light';
      document.documentElement.setAttribute('data-theme', savedTheme);
      const icon = document.querySelector('.theme-toggle i');
      icon.className = savedTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';

      const savedFont = localStorage.getItem('font') || 'ET Book';
      const font = fonts.find(f => f.name === savedFont) || fonts[0];
      document.body.style.fontFamily = font.family;
      updateFontButton(font.name);
    });
  </script>
</body>
</html>
